{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c4d1a39-e1c8-4395-91f6-52ab73f28770",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-07T09:43:36.352783Z",
     "iopub.status.busy": "2023-05-07T09:43:36.352543Z",
     "iopub.status.idle": "2023-05-07T09:43:41.234903Z",
     "shell.execute_reply": "2023-05-07T09:43:41.234055Z",
     "shell.execute_reply.started": "2023-05-07T09:43:36.352757Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahemf/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import evaluate\n",
    "metric = evaluate.load(\"rouge\")\n",
    "from scipy import stats\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "from accelerate.utils import DummyOptim, DummyScheduler\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "from datasets import load_dataset\n",
    "from filelock import FileLock\n",
    "from huggingface_hub import Repository, create_repo\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from copy import deepcopy\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import gc\n",
    "from accelerate import FullyShardedDataParallelPlugin\n",
    "from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model, prepare_model_for_int8_training,  TaskType\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP, StateDictType, FullStateDictConfig\n",
    "from peft.utils.other import fsdp_auto_wrap_policy\n",
    "from datasets import Dataset\n",
    "from datasets import concatenate_datasets\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "from transformers import pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from datasets import Dataset, load_from_disk\n",
    "pd.set_option('max_colwidth', 800)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "def plot_heatmap(df, figsize=(6, 5), fmt='.2f',):\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(240, 10, as_cmap=True)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(df, cmap=cmap, vmax=1.0, vmin=-1.0, cbar_kws={\"shrink\": .8}, center=0,\n",
    "                square=True, linewidths=.5, annot=True, fmt=fmt)\n",
    "    plt.title(\"Column Correlation Heatmap\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_correlation_heatmap(df, threshold=0, figsize=(6, 5), fmt='.2f', spearman=False):\n",
    "    import seaborn as sns\n",
    "    corr = df.corr()\n",
    "    if spearman:\n",
    "        from scipy import stats\n",
    "        res = stats.spearmanr(df.values)\n",
    "        corr = pd.DataFrame(res.statistic, index=corr.index, columns=corr.columns)\n",
    "\n",
    "    # corr = corr.where(np.abs(corr) > threshold, 0)\n",
    "\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.zeros_like(corr, dtype=bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(240, 10, as_cmap=True)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1.0, vmin=-1.0, cbar_kws={\"shrink\": .8}, center=0,\n",
    "                square=True, linewidths=.5, annot=True, fmt=fmt)\n",
    "    plt.title(\"Column Correlation Heatmap\")\n",
    "    plt.show()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    plt.title(\"Columns as heatmap plot\")\n",
    "    sns.heatmap(df, cmap='coolwarm', ax=ax)\n",
    "    plt.show()\n",
    "    \n",
    "    return corr\n",
    "\n",
    "def calculate_jsd(x, y):\n",
    "    if not torch.is_tensor(x):\n",
    "        x = torch.tensor(x)\n",
    "    if not torch.is_tensor(y):\n",
    "        y = torch.tensor(y)\n",
    "    jsd_m = 0.5 * (x + y)\n",
    "    jsd = 0.5 * nn.KLDivLoss(reduction='none', log_target=False)(torch.log(x), jsd_m) + 0.5 * nn.KLDivLoss(reduction='none', log_target=False)(torch.log(y), jsd_m)\n",
    "    jsd = jsd.sum(-1).tolist()\n",
    "    return jsd\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "461827c7-86f7-42ae-b23a-f2fb8ed56891",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T05:09:35.714809Z",
     "iopub.status.busy": "2023-05-10T05:09:35.714433Z",
     "iopub.status.idle": "2023-05-10T05:09:35.726987Z",
     "shell.execute_reply": "2023-05-10T05:09:35.726353Z",
     "shell.execute_reply.started": "2023-05-10T05:09:35.714780Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gender_words = {\n",
    "    'he': 'she',\n",
    " 'him': 'her',\n",
    " 'his': 'hers',\n",
    " 'himself': 'herself',\n",
    " 'son': 'daughter',\n",
    " 'father': 'mother',\n",
    " 'brother': 'sister',\n",
    " 'uncle': 'aunt',\n",
    " 'nephew': 'niece',\n",
    " 'grandfather': 'grandmother',\n",
    " 'husband': 'wife',\n",
    " 'boyfriend': 'girlfriend',\n",
    " 'male': 'female',\n",
    " 'king': 'queen',\n",
    " 'sir': 'madam',\n",
    " 'actor': 'actress',\n",
    " 'host': 'hostess',\n",
    " 'waiter': 'waitress',\n",
    " 'steward': 'stewardess',\n",
    " 'policeman': 'policewoman',\n",
    " 'fireman': 'firewoman',\n",
    " 'chairman': 'chairwoman',\n",
    " 'businessman': 'businesswoman',\n",
    " 'salesman': 'saleswoman',\n",
    " 'doctor': 'doctora',\n",
    " 'mr.': 'ms.',\n",
    " 'groom': 'bride',\n",
    " 'duke': 'duchess',\n",
    " 'hero': 'heroine',\n",
    " 'landlord': 'landlady',\n",
    " 'manager': 'manageress',\n",
    " 'monk': 'nun',\n",
    " 'postman': 'postwoman',\n",
    " 'prince': 'princess',\n",
    " 'prophet': 'prophetess',\n",
    " 'singer': 'songstress',\n",
    " 'sorcerer': 'sorceress',\n",
    " 'waitperson': 'server',\n",
    " 'widower': 'widow',\n",
    " 'she': 'he',\n",
    " 'her': 'him',\n",
    " 'hers': 'his',\n",
    " 'herself': 'himself',\n",
    " 'daughter': 'son',\n",
    " 'mother': 'father',\n",
    " 'sister': 'brother',\n",
    " 'aunt': 'uncle',\n",
    " 'niece': 'nephew',\n",
    " 'grandmother': 'grandfather',\n",
    " 'wife': 'husband',\n",
    " 'girlfriend': 'boyfriend',\n",
    " 'female': 'male',\n",
    " 'queen': 'king',\n",
    " 'madam': 'sir',\n",
    " 'actress': 'actor',\n",
    " 'hostess': 'host',\n",
    " 'waitress': 'waiter',\n",
    " 'stewardess': 'steward',\n",
    " 'policewoman': 'policeman',\n",
    " 'firewoman': 'fireman',\n",
    " 'chairwoman': 'chairman',\n",
    " 'businesswoman': 'businessman',\n",
    " 'saleswoman': 'salesman',\n",
    " 'doctora': 'doctor',\n",
    " 'ms.': 'mr.',\n",
    " 'bride': 'groom',\n",
    " 'duchess': 'duke',\n",
    " 'heroine': 'hero',\n",
    " 'landlady': 'landlord',\n",
    " 'manageress': 'manager',\n",
    " 'nun': 'monk',\n",
    " 'postwoman': 'postman',\n",
    " 'princess': 'prince',\n",
    " 'prophetess': 'prophet',\n",
    " 'songstress': 'singer',\n",
    " 'sorceress': 'sorcerer',\n",
    " 'server': 'waitperson',\n",
    " 'widow': 'widower',\n",
    " 'He': 'She',\n",
    " 'Him': 'Her',\n",
    " 'His': 'Hers',\n",
    " 'Himself': 'Herself',\n",
    " 'Son': 'Daughter',\n",
    " 'Father': 'Mother',\n",
    " 'Brother': 'Sister',\n",
    " 'Uncle': 'Aunt',\n",
    " 'Nephew': 'Niece',\n",
    " 'Grandfather': 'Grandmother',\n",
    " 'Husband': 'Wife',\n",
    " 'Boyfriend': 'Girlfriend',\n",
    " 'Male': 'Female',\n",
    " 'King': 'Queen',\n",
    " 'Sir': 'Madam',\n",
    " 'Actor': 'Actress',\n",
    " 'Host': 'Hostess',\n",
    " 'Waiter': 'Waitress',\n",
    " 'Steward': 'Stewardess',\n",
    " 'Policeman': 'Policewoman',\n",
    " 'Fireman': 'Firewoman',\n",
    " 'Chairman': 'Chairwoman',\n",
    " 'Businessman': 'Businesswoman',\n",
    " 'Salesman': 'Saleswoman',\n",
    " 'Doctor': 'Doctora',\n",
    " 'Mr.': 'Ms.',\n",
    " 'Groom': 'Bride',\n",
    " 'Duke': 'Duchess',\n",
    " 'Hero': 'Heroine',\n",
    " 'Landlord': 'Landlady',\n",
    " 'Manager': 'Manageress',\n",
    " 'Monk': 'Nun',\n",
    " 'Postman': 'Postwoman',\n",
    " 'Prince': 'Princess',\n",
    " 'Prophet': 'Prophetess',\n",
    " 'Singer': 'Songstress',\n",
    " 'Sorcerer': 'Sorceress',\n",
    " 'Waitperson': 'Server',\n",
    " 'Widower': 'Widow',\n",
    " 'She': 'He',\n",
    " 'Her': 'Him',\n",
    " 'Hers': 'His',\n",
    " 'Herself': 'Himself',\n",
    " 'Daughter': 'Son',\n",
    " 'Mother': 'Father',\n",
    " 'Sister': 'Brother',\n",
    " 'Aunt': 'Uncle',\n",
    " 'Niece': 'Nephew',\n",
    " 'Grandmother': 'Grandfather',\n",
    " 'Wife': 'Husband',\n",
    " 'Girlfriend': 'Boyfriend',\n",
    " 'Female': 'Male',\n",
    " 'Queen': 'King',\n",
    " 'Madam': 'Sir',\n",
    " 'Actress': 'Actor',\n",
    " 'Hostess': 'Host',\n",
    " 'Waitress': 'Waiter',\n",
    " 'Stewardess': 'Steward',\n",
    " 'Policewoman': 'Policeman',\n",
    " 'Firewoman': 'Fireman',\n",
    " 'Chairwoman': 'Chairman',\n",
    " 'Businesswoman': 'Businessman',\n",
    " 'Saleswoman': 'Salesman',\n",
    " 'Doctora': 'Doctor',\n",
    " 'Ms.': 'Mr.',\n",
    " 'Bride': 'Groom',\n",
    " 'Duchess': 'Duke',\n",
    " 'Heroine': 'Hero',\n",
    " 'Landlady': 'Landlord',\n",
    " 'Manageress': 'Manager',\n",
    " 'Nun': 'Monk',\n",
    " 'Postwoman': 'Postman',\n",
    " 'Princess': 'Prince',\n",
    " 'Prophetess': 'Prophet',\n",
    " 'Songstress': 'Singer',\n",
    " 'Sorceress': 'Sorcerer',\n",
    " 'Server': 'Waitperson',\n",
    " 'Widow': 'Widower'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "685df068-4a0f-430e-b8a1-aa8254dea067",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T08:51:30.721847Z",
     "iopub.status.busy": "2023-05-08T08:51:30.721442Z",
     "iopub.status.idle": "2023-05-08T08:51:30.726476Z",
     "shell.execute_reply": "2023-05-08T08:51:30.725859Z",
     "shell.execute_reply.started": "2023-05-08T08:51:30.721810Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name=\"t5-large\"\n",
    "dataset_name=\"samsum\"\n",
    "N_FOLD=2\n",
    "\n",
    "max_length = 512\n",
    "max_target_length=128\n",
    "padding=\"max_length\"\n",
    "proba_columns = [\n",
    "                # \"baseline_lora\", \n",
    "                 # \"inverted_jsd\", \n",
    "                 \"proba_v10_cumulative_windowed\", \n",
    "                 \"proba_v11_cumulative_windowed\", \n",
    "                 \"proba_v12_cumulative_windowed\", \n",
    "                 # \"proba_v10_cumulative_windowed_logsig_w10\", \n",
    "                 \"proba_v10_cumulative_windowed_w10\", \n",
    "                 # \"proba_v12_cumulative_windowed_logsig\", \n",
    "                 # \"proba_v10_cumulative_windowed_logsig\", \n",
    "                 # \"proba_v12_cumulative_windowed_logsig_w8\",\n",
    "                 # \"proba_v10_cumulative_logsig_windowed_w8\", \n",
    "                 # \"proba_v12_cumulative_windowed_logsig_w5\", \n",
    "                 # \"proba_v10_cumulative_windowed_logsig_w5\", \n",
    "                 \"proba_v12_cumulative_windowed_w5\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5a4a2733-8ac3-47f0-99f8-3f3fcdab91d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T08:51:33.507721Z",
     "iopub.status.busy": "2023-05-08T08:51:33.507419Z",
     "iopub.status.idle": "2023-05-08T08:51:46.626417Z",
     "shell.execute_reply": "2023-05-08T08:51:46.625420Z",
     "shell.execute_reply.started": "2023-05-08T08:51:33.507694Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahemf/anaconda3/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "WARNING:datasets.builder:Found cached dataset samsum (/home/ahemf/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n",
      "100%|██████████| 3/3 [00:00<00:00, 742.57it/s]\n"
     ]
    }
   ],
   "source": [
    "models = dict()\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "dataset = load_dataset(dataset_name)\n",
    "# models[\"pretrained\"] = model\n",
    "generator=pipeline(task='text2text-generation',model=model,tokenizer=tokenizer, max_length=max_target_length)\n",
    "# generator._forward_params\n",
    "# generator(\"summarize: A: Hi Tom, are you busy tomorrow’s afternoon?\\r\\nB: I’m pretty sure I am. What’s up?\\r\\nA: Can you go with me to the animal shelter?.\\r\\nB: What do you want to do?\\r\\nA: I want to get a puppy for my son.\\r\\nB: That will make him so happy.\\r\\nA: Yeah, we’ve discussed it many times. I think he’s ready now.\\r\\nB: That’s good. Raising a dog is a tough issue. Like having a baby ;-) \\r\\nA: I'll get him one of those little dogs.\\r\\nB: One that won't grow up too big;-)\\r\\nA: And eat too much;-))\\r\\nB: Do you know which one he would like?\\r\\nA: Oh, yes, I took him there last Monday. He showed me one that he really liked.\\r\\nB: I bet you had to drag him away.\\r\\nA: He wanted to take it home right away ;-).\\r\\nB: I wonder what he'll name it.\\r\\nA: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\",\n",
    "#           max_length=10, num_return_sequences=4, num_beams=4, do_sample=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "961335c6-5d88-4adb-9e4e-652e80af1183",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-07T09:43:55.999653Z",
     "iopub.status.busy": "2023-05-07T09:43:55.999336Z",
     "iopub.status.idle": "2023-05-07T09:43:56.092064Z",
     "shell.execute_reply": "2023-05-07T09:43:56.091435Z",
     "shell.execute_reply.started": "2023-05-07T09:43:55.999627Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26730"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "ctr = Counter([w for sent in map(lambda x: x.strip().split(), list(dataset[\"train\"][\"summary\"])) for w in sent])\n",
    "len(ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "80b50ece-e1a2-41f9-ade6-d2d1ea0704c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T08:51:48.141695Z",
     "iopub.status.busy": "2023-05-08T08:51:48.141446Z",
     "iopub.status.idle": "2023-05-08T08:51:54.630404Z",
     "shell.execute_reply": "2023-05-08T08:51:54.629711Z",
     "shell.execute_reply.started": "2023-05-08T08:51:48.141668Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model\n",
    "for proba_col in proba_columns: \n",
    "    \n",
    "    md = deepcopy(model)\n",
    "    if \"_lora\" not in proba_col:\n",
    "        state_dict = torch.load(f\"outputs/{model_name}/{dataset_name}/folds_{N_FOLD}_{proba_col}_combined/model.pt\", map_location='cpu')\n",
    "        md.load_state_dict(state_dict)\n",
    "    else:\n",
    "        \n",
    "        peft_config = PeftConfig.from_pretrained(f\"outputs/{model_name}/{dataset_name}/{proba_col}\")\n",
    "        md = PeftModel.from_pretrained(md, f\"outputs/{model_name}/{dataset_name}/{proba_col}\")\n",
    "    md = md.eval()\n",
    "    models[proba_col] = md\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "83ad5a84-4177-425c-a1c1-7bfa653c9ed3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T08:48:46.312942Z",
     "iopub.status.busy": "2023-05-08T08:48:46.312696Z",
     "iopub.status.idle": "2023-05-08T08:48:46.316098Z",
     "shell.execute_reply": "2023-05-08T08:48:46.315508Z",
     "shell.execute_reply.started": "2023-05-08T08:48:46.312916Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/blob/v4.28.1/src/transformers/pipelines/text2text_generation.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c826ec15-6afe-4488-a260-93931f6b4d96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T08:51:54.632213Z",
     "iopub.status.busy": "2023-05-08T08:51:54.631963Z",
     "iopub.status.idle": "2023-05-08T08:51:58.306883Z",
     "shell.execute_reply": "2023-05-08T08:51:58.306072Z",
     "shell.execute_reply.started": "2023-05-08T08:51:54.632187Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /local/home/ahemf/mygit/prevent-memorisation/outputs/t5-large/samsum/folds_2_jsd/cache-8b9e7d35826aa143.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsets = []\n",
    "\n",
    "for FOLD in range(N_FOLD):\n",
    "    ds = Dataset.load_from_disk(f\"outputs/{model_name}/{dataset_name}/fold_{N_FOLD}_{FOLD}\")\n",
    "    ds = ds.rename_column(\"proba\", f\"proba{FOLD}\")\n",
    "    md = deepcopy(model)\n",
    "    md.load_state_dict(torch.load(f\"outputs/{model_name}/{dataset_name}/fold_{N_FOLD}_{FOLD}/model.pt\", map_location = 'cpu'))\n",
    "    md = md.eval()\n",
    "    models[FOLD] = md\n",
    "    \n",
    "    dsets.append(ds)\n",
    "    \n",
    "combined_ds = Dataset.load_from_disk(f\"outputs/{model_name}/{dataset_name}/folds_{N_FOLD}_combined\")\n",
    "combined_ds_jsd = Dataset.load_from_disk(f\"outputs/{model_name}/{dataset_name}/folds_{N_FOLD}_jsd\")\n",
    "combined_ds_jsd = combined_ds_jsd.map(lambda x:{k: v + [v[-1]]*2 for k, v in x.items()})\n",
    "combined_ds_jsd = combined_ds_jsd.rename_column(\"proba0\", \"proba0_jsd\").rename_column(\"proba1\", \"proba1_jsd\")\n",
    "combined_ds = concatenate_datasets([combined_ds, combined_ds_jsd], axis=1)\n",
    "\n",
    "md = deepcopy(model)\n",
    "md.load_state_dict(torch.load(f\"outputs/{model_name}/{dataset_name}/baseline/model.pt\", map_location = 'cpu'))\n",
    "models[\"baseline\"] = md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ef8bb7e-b8b3-4dab-a15e-e4c5c6efda64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-07T09:44:29.703056Z",
     "iopub.status.busy": "2023-05-07T09:44:29.702810Z",
     "iopub.status.idle": "2023-05-07T09:44:29.706633Z",
     "shell.execute_reply": "2023-05-07T09:44:29.706011Z",
     "shell.execute_reply.started": "2023-05-07T09:44:29.703031Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combined_ds.save_to_disk(f\"outputs/{model_name}/{dataset_name}/folds_{N_FOLD}_combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a7814e31-8351-46b2-9c67-d8f58a224f41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-07T13:47:45.750551Z",
     "iopub.status.busy": "2023-05-07T13:47:45.750159Z",
     "iopub.status.idle": "2023-05-07T13:47:51.844478Z",
     "shell.execute_reply": "2023-05-07T13:47:51.843440Z",
     "shell.execute_reply.started": "2023-05-07T13:47:45.750523Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting line_profiler\n",
      "  Downloading line_profiler-4.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (661 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.9/661.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: line_profiler\n",
      "Successfully installed line_profiler-4.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f4942d5d-7d6d-4edd-a004-643206b9e317",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-07T13:47:57.982300Z",
     "iopub.status.busy": "2023-05-07T13:47:57.981914Z",
     "iopub.status.idle": "2023-05-07T13:47:57.999199Z",
     "shell.execute_reply": "2023-05-07T13:47:57.998333Z",
     "shell.execute_reply.started": "2023-05-07T13:47:57.982273Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7ea0517a-6755-4dd3-9487-43ab30ab7ff7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T17:57:41.390501Z",
     "iopub.status.busy": "2023-05-09T17:57:41.390105Z",
     "iopub.status.idle": "2023-05-09T17:57:41.396962Z",
     "shell.execute_reply": "2023-05-09T17:57:41.396293Z",
     "shell.execute_reply.started": "2023-05-09T17:57:41.390472Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "def get_one_step_proba(model, labels, input_ids, attention_mask, encoder_outputs=None):\n",
    "    original_lables = deepcopy(labels)\n",
    "    # labels = model._shift_right(labels)\n",
    "    probas = []\n",
    "    log_proba = 0\n",
    "    logits = []\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask, labels=deepcopy(labels), encoder_outputs=encoder_outputs)\n",
    "        lm_logits = outputs.logits.softmax(dim=-1).squeeze(0)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        for i in range(labels.shape[-1]-1):\n",
    "            actual = original_lables[..., :(i+1)]\n",
    "            proba = lm_logits[i, actual[0, -1]].item()\n",
    "            if actual[0, -1].item()!=0:\n",
    "                log_proba += np.log(proba)\n",
    "            probas.append(proba)\n",
    "        return {\"probas\": probas, \"loss\": loss.item(), \"log_proba\": log_proba}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "f9682cca-8901-485f-b2f3-194c31817d47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T05:54:08.988570Z",
     "iopub.status.busy": "2023-05-10T05:54:08.988135Z",
     "iopub.status.idle": "2023-05-10T05:54:08.997741Z",
     "shell.execute_reply": "2023-05-10T05:54:08.997112Z",
     "shell.execute_reply.started": "2023-05-10T05:54:08.988542Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def greedy_prefix_decoding(model, tokenizer, prefix_text, input_ids, attention_mask, max_length, encoder_outputs=None):\n",
    "    eos_token_id, pad_token_id = tokenizer.eos_token_id, tokenizer.pad_token_id\n",
    "    prefixs = tokenizer(text_target=[prefix_text], max_length=max_length, padding=\"do_not_pad\", \n",
    "                    truncation=True, return_tensors=\"pt\", \n",
    "                    add_special_tokens=False)\n",
    "    prefix_ids = prefixs[\"input_ids\"].type(torch.int).to(input_ids.device)\n",
    "    out_token_ids = deepcopy(prefix_ids)\n",
    "    \n",
    "    # prefix_ids = model._shift_right(prefix_ids)\n",
    "    start_pad = torch.tensor([pad_token_id], device=prefix_ids.device).unsqueeze(0)\n",
    "    prefix_ids = torch.cat([start_pad, prefix_ids], dim=1)\n",
    "    assert prefix_ids.shape[0] == 1\n",
    "    with torch.no_grad():\n",
    "        if encoder_outputs is None:\n",
    "            encoder_outputs = model.encoder(input_ids, attention_mask)\n",
    "        encoder_hidden_states = encoder_outputs[0]\n",
    "        \n",
    "        past_key_values = None\n",
    "        next_token = None\n",
    "        gen_len = 0\n",
    "        while next_token is None or next_token[0].item() != eos_token_id and out_token_ids.shape[1] < max_length:\n",
    "            # print(prefix_ids, tokenizer.batch_decode(prefix_ids))\n",
    "            with torch.autocast(\"cuda\"):\n",
    "                decoder_outputs = model.decoder(input_ids=prefix_ids, past_key_values=past_key_values, \n",
    "                                                encoder_hidden_states=encoder_hidden_states, \n",
    "                                                encoder_attention_mask=attention_mask, use_cache=True)\n",
    "            past_key_values = decoder_outputs.past_key_values\n",
    "            gen_len += 1\n",
    "            if gen_len % 32 == 0:\n",
    "                past_key_values = None\n",
    "            sequence_output = decoder_outputs[0]\n",
    "            if model.config.tie_word_embeddings:\n",
    "                sequence_output = sequence_output * (model.model_dim**-0.5)\n",
    "            lm_logits = model.lm_head(sequence_output).softmax(dim=-1) # B, S, D\n",
    "            next_token_logits = lm_logits[:, -1] # B, D\n",
    "            next_token = torch.argmax(next_token_logits, dim=1).unsqueeze(1) # B, 1\n",
    "            out_token_ids = torch.cat([out_token_ids, next_token], dim=1)\n",
    "            prefix_ids = torch.cat([prefix_ids, next_token], dim=1)\n",
    "        return out_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8adfbee2-30a3-4ead-a288-8b7a20bd9845",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T05:00:47.260120Z",
     "iopub.status.busy": "2023-05-10T05:00:47.259734Z",
     "iopub.status.idle": "2023-05-10T05:00:47.266534Z",
     "shell.execute_reply": "2023-05-10T05:00:47.265887Z",
     "shell.execute_reply.started": "2023-05-10T05:00:47.260091Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ahemf/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Noising (He to She), Double stopwords mistake\n",
    "# Invert gender, \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "7a6a6baf-722c-4982-b271-7371a4c0d841",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T11:57:20.724399Z",
     "iopub.status.busy": "2023-05-10T11:57:20.723991Z",
     "iopub.status.idle": "2023-05-10T11:57:20.749020Z",
     "shell.execute_reply": "2023-05-10T11:57:20.748207Z",
     "shell.execute_reply.started": "2023-05-10T11:57:20.724326Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_for_one_model(model, model_key, idx, input_text, label_text, tokenizer, padding, device, **gen_kwargs):\n",
    "    _ = gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    label_ids = tokenizer(text_target=[label_text], max_length=gen_kwargs[\"max_length\"], padding=padding, truncation=True)[\"input_ids\"][0]\n",
    "    words = label_text.split()\n",
    "    batch = tokenizer(input_text, max_length=max_length, padding=padding, truncation=True, return_tensors=\"pt\")\n",
    "    model_texts = dict()\n",
    "    model_texts[\"idx\"] = idx\n",
    "    model_texts[\"label\"] = label_text\n",
    "    model_texts[\"model_key\"] = model_key\n",
    "    batch[\"input_ids\"] = batch[\"input_ids\"].to(device)\n",
    "    batch[\"attention_mask\"] = batch[\"attention_mask\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.encoder(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "        generated_ids = model.generate(\n",
    "                            input_ids=batch[\"input_ids\"],\n",
    "                            encoder_outputs=encoder_outputs,\n",
    "                            attention_mask=batch[\"attention_mask\"], \n",
    "                            use_cache=True,\n",
    "                            **gen_kwargs,\n",
    "                        )\n",
    "        one_steps_probas = get_one_step_proba(model, \n",
    "                                              tokenizer(text_target=[label_text], max_length=gen_kwargs[\"max_length\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\")[\"input_ids\"].to(device), \n",
    "                                              batch[\"input_ids\"], batch[\"attention_mask\"], encoder_outputs=encoder_outputs,)\n",
    "        log_proba_of_label = one_steps_probas[\"log_proba\"]\n",
    "        model_texts[\"log_proba_of_label\"] = log_proba_of_label\n",
    "        generated_ids = generated_ids.squeeze().tolist()\n",
    "        if generated_ids[0] == tokenizer.pad_token_id:\n",
    "            generated_ids = generated_ids[1:]\n",
    "        predictions = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "        model_texts[\"predictions\"] = predictions\n",
    "        rouge_score = metric.compute(predictions=[predictions], references=[label_text], use_stemmer=True)\n",
    "        model_texts[\"rouge_score\"] = rouge_score\n",
    "        \n",
    "    # Get proba of label\n",
    "    # inputs_embeds\n",
    "    \n",
    "    model_texts[\"word_index_list\"] = list(range(1, len(words) - 1))\n",
    "    for i in range(1, len(words) - 1):\n",
    "        _ = gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        prefix_text = \" \".join(words[:i])\n",
    "        model_texts[i] = dict()\n",
    "        model_texts[i][\"prefix_text\"] = prefix_text\n",
    "        greedy_prefix_decode = greedy_prefix_decoding(model, \n",
    "                                                      tokenizer, \n",
    "                                                      prefix_text, \n",
    "                                                      batch[\"input_ids\"].type(torch.int), \n",
    "                                                      batch[\"attention_mask\"].type(torch.int), \n",
    "                                                      max_target_length, encoder_outputs=encoder_outputs,).squeeze().tolist()\n",
    "        greedy_prefix_predictions = tokenizer.decode(greedy_prefix_decode, skip_special_tokens=True)\n",
    "        model_texts[i][\"predictions\"] = greedy_prefix_predictions\n",
    "        rouge_score = metric.compute(predictions=[greedy_prefix_predictions], references=[label_text], use_stemmer=True)\n",
    "        model_texts[i][\"rouge_score\"] = rouge_score\n",
    "        model_texts[i][\"prefix_text_match_label\"] = greedy_prefix_predictions.strip().lower() == label_text.strip().lower()\n",
    "        generation_length_verbatim = 0\n",
    "        generation_suffix_verbatim_text = \"\"\n",
    "        gen_words = greedy_prefix_predictions.lower().split()\n",
    "        for aw, gw in list(zip(label_text.lower().split(), gen_words))[i:]:\n",
    "            if aw != gw:\n",
    "                break\n",
    "            generation_length_verbatim += 1\n",
    "            generation_suffix_verbatim_text += (gw+\" \")\n",
    "        generation_suffix_verbatim_text = generation_suffix_verbatim_text.strip()\n",
    "        model_texts[i][\"generation_length_verbatim\"] = generation_length_verbatim\n",
    "        model_texts[i][\"generation_suffix_verbatim_text\"] = generation_suffix_verbatim_text\n",
    "        model_texts[i][\"generation_length_verbatim_by_prefix_length\"] = generation_length_verbatim / i\n",
    "        model_texts[i][\"generation_length_verbatim_by_generation_length\"] = generation_length_verbatim / len(gen_words)\n",
    "        model_texts[i][\"generation_length_verbatim_by_label_length\"] = generation_length_verbatim / len(words)\n",
    "        \n",
    "        if words[:i][-1].lower() in stop_words and False:\n",
    "            model_texts[i][\"stopword_repeat\"] = dict()\n",
    "            model_texts[i][\"stopword_repeat\"][\"stopword\"] = words[:i][-1]\n",
    "            prefix_text = \" \".join(words[:i] + [words[:i][-1]])\n",
    "            greedy_prefix_decode = greedy_prefix_decoding(model, \n",
    "                                                      tokenizer, \n",
    "                                                      prefix_text, \n",
    "                                                      batch[\"input_ids\"].type(torch.int), \n",
    "                                                      batch[\"attention_mask\"].type(torch.int), \n",
    "                                                      max_target_length, encoder_outputs=encoder_outputs,).squeeze().tolist()\n",
    "            greedy_prefix_predictions = tokenizer.decode(greedy_prefix_decode, skip_special_tokens=True)\n",
    "            model_texts[i][\"stopword_repeat\"][\"prefix_text\"] = prefix_text\n",
    "            model_texts[i][\"stopword_repeat\"][\"predictions\"] = greedy_prefix_predictions\n",
    "            generation_length_verbatim = 0\n",
    "            generation_suffix_verbatim_text = \"\"\n",
    "            gen_words = greedy_prefix_predictions.lower().split()\n",
    "            for aw, gw in list(zip(label_text.lower().split()[i:], gen_words[i+1:])):\n",
    "\n",
    "                if aw != gw:\n",
    "                    break\n",
    "                generation_length_verbatim += 1\n",
    "                generation_suffix_verbatim_text += (gw+\" \")\n",
    "            generation_suffix_verbatim_text = generation_suffix_verbatim_text.strip()\n",
    "            model_texts[i][\"stopword_repeat\"][\"generation_length_verbatim\"] = generation_length_verbatim\n",
    "            model_texts[i][\"stopword_repeat\"][\"generation_suffix_verbatim_text\"] = generation_suffix_verbatim_text\n",
    "            model_texts[i][\"stopword_repeat\"][\"generation_length_verbatim_by_prefix_length\"] = generation_length_verbatim / i\n",
    "            model_texts[i][\"stopword_repeat\"][\"generation_length_verbatim_by_generation_length\"] = generation_length_verbatim / len(gen_words)\n",
    "            model_texts[i][\"stopword_repeat\"][\"generation_length_verbatim_by_label_length\"] = generation_length_verbatim / len(words)\n",
    "        if words[:i][-1].lower() in gender_words:\n",
    "            model_texts[i][\"gender_swap\"] = dict()\n",
    "            model_texts[i][\"gender_swap\"][\"stopword\"] = gender_words[words[:i][-1]]\n",
    "            prefix_text = \" \".join(words[:i-1] + [gender_words[words[:i][-1]]])\n",
    "            greedy_prefix_decode = greedy_prefix_decoding(model, \n",
    "                                                      tokenizer, \n",
    "                                                      prefix_text, \n",
    "                                                      batch[\"input_ids\"].type(torch.int), \n",
    "                                                      batch[\"attention_mask\"].type(torch.int), \n",
    "                                                      max_target_length, encoder_outputs=encoder_outputs,).squeeze().tolist()\n",
    "            greedy_prefix_predictions = tokenizer.decode(greedy_prefix_decode, skip_special_tokens=True)\n",
    "            model_texts[i][\"gender_swap\"][\"prefix_text\"] = prefix_text\n",
    "            model_texts[i][\"gender_swap\"][\"predictions\"] = greedy_prefix_predictions\n",
    "            generation_length_verbatim = 0\n",
    "            generation_suffix_verbatim_text = \"\"\n",
    "            gen_words = greedy_prefix_predictions.lower().split()\n",
    "            for aw, gw in list(zip(label_text.lower().split(), gen_words))[i:]:\n",
    "\n",
    "                if aw != gw:\n",
    "                    break\n",
    "                generation_length_verbatim += 1\n",
    "                generation_suffix_verbatim_text += (gw+\" \")\n",
    "            generation_suffix_verbatim_text = generation_suffix_verbatim_text.strip()\n",
    "            model_texts[i][\"gender_swap\"][\"generation_length_verbatim\"] = generation_length_verbatim\n",
    "            model_texts[i][\"gender_swap\"][\"generation_suffix_verbatim_text\"] = generation_suffix_verbatim_text\n",
    "            model_texts[i][\"gender_swap\"][\"generation_length_verbatim_by_prefix_length\"] = generation_length_verbatim / i\n",
    "            model_texts[i][\"gender_swap\"][\"generation_length_verbatim_by_generation_length\"] = generation_length_verbatim / len(gen_words)\n",
    "            model_texts[i][\"gender_swap\"][\"generation_length_verbatim_by_label_length\"] = generation_length_verbatim / len(words)\n",
    "    return model_texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "e94e10ab-78d7-4f3f-841c-4c5e57cf5c13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T11:56:24.944568Z",
     "iopub.status.busy": "2023-05-10T11:56:24.944142Z",
     "iopub.status.idle": "2023-05-10T11:56:25.544374Z",
     "shell.execute_reply": "2023-05-10T11:56:25.543546Z",
     "shell.execute_reply.started": "2023-05-10T11:56:24.944538Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models[\"proba_v10_cumulative_windowed_w10\"] = models[\"proba_v10_cumulative_windowed_w10\"].to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "d8856c49-140a-429e-864c-47066bcb17c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T11:56:32.864678Z",
     "iopub.status.busy": "2023-05-10T11:56:32.864196Z",
     "iopub.status.idle": "2023-05-10T11:56:33.409753Z",
     "shell.execute_reply": "2023-05-10T11:56:33.408827Z",
     "shell.execute_reply.started": "2023-05-10T11:56:32.864647Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device1 = torch.device(\"cuda:1\")\n",
    "models[1]=models[1].to(\"cpu\")\n",
    "models[0]=models[0].to(\"cpu\")\n",
    "models[\"baseline\"] = models[\"baseline\"].to(device1)\n",
    "device2 = torch.device(\"cuda:2\")\n",
    "models[\"proba_v12_cumulative_windowed\"] = models[\"proba_v12_cumulative_windowed\"].to(device2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "8127a1f6-0d9a-46f1-a544-93d953802058",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T11:58:05.835074Z",
     "iopub.status.busy": "2023-05-10T11:58:05.834711Z",
     "iopub.status.idle": "2023-05-10T12:26:08.751725Z",
     "shell.execute_reply": "2023-05-10T12:26:08.751017Z",
     "shell.execute_reply.started": "2023-05-10T11:58:05.835045Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [28:02<00:00, 56.10s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "results = []\n",
    "for idx in trange(0, 30):\n",
    "    label_text = dataset[\"train\"][idx][\"summary\"]\n",
    "    input_text = f'summarize: {dataset[\"train\"][idx][\"dialogue\"]}'\n",
    "    r1 = run_for_one_model(models[\"baseline\"], \"baseline\", idx, input_text, label_text, tokenizer, padding, device1, **gen_kwargs)\n",
    "    r2 = run_for_one_model(models[\"proba_v12_cumulative_windowed\"], \"proba_v12_cumulative_windowed\", idx, input_text, label_text, tokenizer, padding, device2, **gen_kwargs)\n",
    "    results.append([r1, r2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "fbb7c4f6-a48b-4c9c-90b1-a33b916a083d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T08:04:27.232394Z",
     "iopub.status.busy": "2023-05-11T08:04:27.231980Z",
     "iopub.status.idle": "2023-05-11T08:13:35.978961Z",
     "shell.execute_reply": "2023-05-11T08:13:35.977725Z",
     "shell.execute_reply.started": "2023-05-11T08:04:27.232326Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [09:08<09:08, 54.86s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in trange(91, 101):\n",
    "    label_text = dataset[\"train\"][idx][\"summary\"]\n",
    "    input_text = f'summarize: {dataset[\"train\"][idx][\"dialogue\"]}'\n",
    "    r1 = run_for_one_model(models[\"baseline\"], \"baseline\", idx, input_text, label_text, tokenizer, padding, device1, **gen_kwargs)\n",
    "    r2 = run_for_one_model(models[\"proba_v12_cumulative_windowed\"], \"proba_v12_cumulative_windowed\", idx, input_text, label_text, tokenizer, padding, device2, **gen_kwargs)\n",
    "    results.append([r1, r2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "58da9641-5ea5-48e4-be2d-71cae482acc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T08:18:37.325223Z",
     "iopub.status.busy": "2023-05-11T08:18:37.324849Z",
     "iopub.status.idle": "2023-05-11T08:18:37.334645Z",
     "shell.execute_reply": "2023-05-11T08:18:37.333907Z",
     "shell.execute_reply.started": "2023-05-11T08:18:37.325194Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-264 0 -20 -1.4828793558960458\n"
     ]
    }
   ],
   "source": [
    "normal_verbatims = 0\n",
    "stopword_verbatims = 0\n",
    "gender_verbatims = 0\n",
    "log_proba_diff = 0\n",
    "for i, (r1, r2) in enumerate(results):\n",
    "    wl = r1[\"word_index_list\"]\n",
    "    log_proba_diff += ((r2[\"log_proba_of_label\"] - r1[\"log_proba_of_label\"])/len(wl))\n",
    "    for ix in wl:\n",
    "        normal_verbatims += (r2[ix][\"generation_length_verbatim\"] - r1[ix][\"generation_length_verbatim\"])\n",
    "        \n",
    "        # if \"stopword_repeat\" in r1[ix]:\n",
    "        #     stopword_verbatims+= (r2[ix][\"stopword_repeat\"][\"generation_length_verbatim\"] - r1[ix][\"stopword_repeat\"][\"generation_length_verbatim\"])\n",
    "        if \"gender_swap\" in r1[ix]:\n",
    "            gender_verbatims += (r2[ix][\"gender_swap\"][\"generation_length_verbatim\"] - r1[ix][\"gender_swap\"][\"generation_length_verbatim\"])\n",
    "print(normal_verbatims, stopword_verbatims, gender_verbatims, log_proba_diff/len(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "ea353d8e-164b-4d42-8f76-d807e1477dae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T08:28:05.324954Z",
     "iopub.status.busy": "2023-05-11T08:28:05.324384Z",
     "iopub.status.idle": "2023-05-11T08:28:05.344949Z",
     "shell.execute_reply": "2023-05-11T08:28:05.344099Z",
     "shell.execute_reply.started": "2023-05-11T08:28:05.324924Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amanda baked cookies and will bring Jerry some tomorrow. \n",
      " Amanda baked cookies and will bring them to Jerry tomorrow. \n",
      " Amanda baked cookies and || will bring || will bring jerry some tomorrow.\n",
      "========================================\n",
      "Amanda baked cookies and will bring Jerry some tomorrow. \n",
      " Amanda baked cookies and will bring them to Jerry tomorrow. \n",
      " Amanda baked cookies and will || bring || bring jerry some tomorrow.\n",
      "========================================\n",
      "Amanda baked cookies and will bring Jerry some tomorrow. \n",
      " Amanda baked cookies and will bring them to Jerry tomorrow. \n",
      " Amanda baked cookies and will bring ||  || jerry some tomorrow.\n",
      "========================================\n",
      "Kim may try the pomodoro technique recommended by Tim to get more stuff done. \n",
      " Kim may move her ass tomorrow. \n",
      " Kim may ||  || try the pomodoro technique\n",
      "========================================\n",
      "Kim may try the pomodoro technique recommended by Tim to get more stuff done. \n",
      " Kim may try to do everything tomorrow. \n",
      " Kim may try ||  || the pomodoro technique\n",
      "========================================\n",
      "Sam is confused, because he overheard Rick complaining about him as a roommate. Naomi thinks Sam should talk to Rick. Sam is not sure what to do. \n",
      " Sam is confused, because rick said that he doesn't likes he doesn's's he doesn's not like being his roommate. \n",
      " Sam is confused, || because || because he overheard rick\n",
      "========================================\n",
      "Sam is confused, because he overheard Rick complaining about him as a roommate. Naomi thinks Sam should talk to Rick. Sam is not sure what to do. \n",
      " Sam is confused, because Rick said that he doesn't like being his roommate is not like being his roommate is not like being his. \n",
      " Sam is confused, because ||  || he overheard rick\n",
      "========================================\n",
      "Sam is confused, because he overheard Rick complaining about him as a roommate. Naomi thinks Sam should talk to Rick. Sam is not sure what to do. \n",
      " Sam is confused, because he overheard Rick saying that he doesn't he doesn's. \n",
      " Sam is confused, because he overheard Rick ||  || complaining about\n",
      "========================================\n",
      "Wyatt reminds Neville his wedding anniversary is on the 17th of September. Neville's wife is upset and it might be because Neville forgot about their anniversary. \n",
      " Wyatt reminds Neville his wedding anniversary was on September 17. \n",
      " Wyatt reminds Neville his wedding anniversary ||  || is on\n",
      "========================================\n",
      "Wyatt reminds Neville his wedding anniversary is on the 17th of September. Neville's wife is upset and it might be because Neville forgot about their anniversary. \n",
      " Wyatt reminds Neville his wedding anniversary is on the 17th of September. Neville's wife is upset and it might have something to \n",
      " Wyatt reminds Neville his wedding anniversary is on the 17th of September. Neville's wife is upset and it might ||  || be\n",
      "========================================\n",
      "John didn't show up for class due to some work issues with his boss. Cassandra, his teacher told him which exercises to do, and which chapter to study. They are going to meet up for a beer sometime this week after class.  \n",
      " John didn't show up for class due to some work. Cassandra will help from Cas. \n",
      " John didn't show up for class due to some work ||  || issues with his boss.\n",
      "========================================\n",
      "John didn't show up for class due to some work issues with his boss. Cassandra, his teacher told him which exercises to do, and which chapter to study. They are going to meet up for a beer sometime this week after class.  \n",
      " John didn't show up for class due to some work issues with his boss. Cassandra, his teacher told him which exercises to do, and they will help him. \n",
      " John didn't show up for class due to some work issues with his boss. Cassandra, his teacher told him which exercises to do, || and || and which chapter\n",
      "========================================\n",
      "John didn't show up for class due to some work issues with his boss. Cassandra, his teacher told him which exercises to do, and which chapter to study. They are going to meet up for a beer sometime this week after class.  \n",
      " John didn't show up for class due to some work issues with his boss. Cassandra, his teacher told him which exercises to do, and he will help him. \n",
      " John didn't show up for class due to some work issues with his boss. Cassandra, his teacher told him which exercises to do, and ||  || which chapter to\n",
      "========================================\n",
      "John didn't show up for class due to some work issues with his boss. Cassandra, his teacher told him which exercises to do, and which chapter to study. They are going to meet up for a beer sometime this week after class.  \n",
      " John didn't show up for class due to some work issues with his boss. Cassandra, his teacher told him which exercises to do, and which ones to study for the others to do. \n",
      " John didn't show up for class due to some work issues with his boss. Cassandra, his teacher told him which exercises to do, and which ||  || chapter to\n",
      "========================================\n",
      "John didn't show up for class due to some work issues with his boss. Cassandra, his teacher told him which exercises to do, and which chapter to study. They are going to meet up for a beer sometime this week after class.  \n",
      " John didn't show up for class due to some work issues with his boss. Cassandra, his teacher told him which exercises to do, and which chapter to study. They are going to meet up for a beer sometime this week. \n",
      " John didn't show up for class due to some work issues with his boss. Cassandra, his teacher told him which exercises to do, and which chapter to study. They are going to meet up for a beer sometime this ||  || week after\n",
      "========================================\n",
      "Sarah sends James an instrumental song he might like. James knows the song. The brain connects the songs to the context they were played in and brings to mind the associated memories. \n",
      " Sarah sends James an instrumental song he might like. James knows the song. The activity your brain does when she will reminds of her partner. \n",
      " Sarah sends James an instrumental song he might like. James knows the song. The ||  || brain connects the\n",
      "========================================\n",
      "Sarah sends James an instrumental song he might like. James knows the song. The brain connects the songs to the context they were played in and brings to mind the associated memories. \n",
      " Sarah sends James an instrumental song he might like. James knows the song. The brain will connect to the song will be thinking of his work. \n",
      " Sarah sends James an instrumental song he might like. James knows the song. The brain ||  || connects the\n",
      "========================================\n",
      "Sarah sends James an instrumental song he might like. James knows the song. The brain connects the songs to the context they were played in and brings to mind the associated memories. \n",
      " Sarah sends James an instrumental song he might like. James knows the song. The brain connects to the song when she likes to the song. \n",
      " Sarah sends James an instrumental song he might like. James knows the song. The brain connects ||  || the\n",
      "========================================\n",
      "Sarah sends James an instrumental song he might like. James knows the song. The brain connects the songs to the context they were played in and brings to mind the associated memories. \n",
      " Sarah sends James an instrumental song he might like. James knows the song. The brain connects the songs to work. \n",
      " Sarah sends James an instrumental song he might like. James knows the song. The brain connects the songs to ||  || the\n",
      "========================================\n",
      "Sarah sends James an instrumental song he might like. James knows the song. The brain connects the songs to the context they were played in and brings to mind the associated memories. \n",
      " Sarah sends James an instrumental song he might like. James knows the song. The brain connects the songs to the context of the listen \n",
      " Sarah sends James an instrumental song he might like. James knows the song. The brain connects the songs to the context ||  || they\n",
      "========================================\n",
      "Sarah sends James an instrumental song he might like. James knows the song. The brain connects the songs to the context they were played in and brings to mind the associated memories. \n",
      " Sarah sends James an instrumental song he might like. James knows the song. The brain connects the songs to the context they were played in and brings to mind work. \n",
      " Sarah sends James an instrumental song he might like. James knows the song. The brain connects the songs to the context they were played in and brings to mind ||  || the\n",
      "========================================\n",
      "Noah wants to meet, he quit his job, because his boss was a dick. \n",
      " Noah wants to meet, he quit his job. \n",
      " Noah wants to meet, he || quit his || quit his job, because his boss\n",
      "========================================\n",
      "Noah wants to meet, he quit his job, because his boss was a dick. \n",
      " Noah wants to meet, he quit his job. \n",
      " Noah wants to meet, he quit || his || his job, because his boss\n",
      "========================================\n",
      "Noah wants to meet, he quit his job, because his boss was a dick. \n",
      " Noah wants to meet, he quit his job, and he will meet with Madison. \n",
      " Noah wants to meet, he quit his job, ||  || because his boss\n",
      "========================================\n",
      "Noah wants to meet, he quit his job, because his boss was a dick. \n",
      " Noah wants to meet, he quit his job, because his boss turned into a cocky. \n",
      " Noah wants to meet, he quit his job, because his boss ||  || was a\n",
      "========================================\n",
      "Matt invites Agnes for a date to get to know each other better. They'll go to the Georgian restaurant in Kazimierz on Saturday at 6 pm, and he'll pick her up on the way to the place. \n",
      " Matt invites Agnes for a date on Saturday at 6 pm to the Georgian \n",
      " Matt invites || agnes for a date || agnes for a date to\n",
      "========================================\n",
      "Matt invites Agnes for a date to get to know each other better. They'll go to the Georgian restaurant in Kazimierz on Saturday at 6 pm, and he'll pick her up on the way to the place. \n",
      " Matt invites Agnes for a date on Saturday at 6 pm to the Georgian \n",
      " Matt invites Agnes || for a date || for a date to\n",
      "========================================\n",
      "Matt invites Agnes for a date to get to know each other better. They'll go to the Georgian restaurant in Kazimierz on Saturday at 6 pm, and he'll pick her up on the way to the place. \n",
      " Matt invites Agnes for a date on Saturday at 6 pm to the Georgian \n",
      " Matt invites Agnes for || a date || a date to\n",
      "========================================\n",
      "Matt invites Agnes for a date to get to know each other better. They'll go to the Georgian restaurant in Kazimierz on Saturday at 6 pm, and he'll pick her up on the way to the place. \n",
      " Matt invites Agnes for a date on Saturday at 6 pm to the Georgian \n",
      " Matt invites Agnes for a || date || date to\n",
      "========================================\n",
      "Matt invites Agnes for a date to get to know each other better. They'll go to the Georgian restaurant in Kazimierz on Saturday at 6 pm, and he'll pick her up on the way to the place. \n",
      " Matt invites Agnes for a date on Saturday at 6 pm to the Georgian \n",
      " Matt invites Agnes for a date ||  || to\n",
      "========================================\n",
      "Matt invites Agnes for a date to get to know each other better. They'll go to the Georgian restaurant in Kazimierz on Saturday at 6 pm, and he'll pick her up on the way to the place. \n",
      " Matt invites Agnes for a date to get to know each other better. They'll meet at the Georgia \n",
      " Matt invites Agnes for a date to get to know each other better. They'll ||  || go to\n",
      "========================================\n",
      "Demi got promoted. She will celebrate that with Lucas at Death & Co at 10 pm. \n",
      " Demi got promoted. She will see Lucas and Lucas at Death &Co. \n",
      " Demi got promoted. || she will || she will celebrate\n",
      "========================================\n",
      "Demi got promoted. She will celebrate that with Lucas at Death & Co at 10 pm. \n",
      " Demi got promoted. She will celebrate that with Lucas at Death & Co. tonight at 10 pm \n",
      " Demi got promoted. She will celebrate that with Lucas at Death & Co ||  || at 10 pm.\n",
      "========================================\n",
      "Demi got promoted. She will celebrate that with Lucas at Death & Co at 10 pm. \n",
      " Demi got promoted. She will celebrate that with Lucas at Death & Co at 10 pm tonight. \n",
      " Demi got promoted. She will celebrate that with Lucas at Death & Co at || 10 || 10 pm.\n",
      "========================================\n",
      "Arthur is still unemployed. Leon sends him a job offer for junior project manager position. Arthur is interested. \n",
      " Arthur is still unemployed. Leon sends him a resume to his friend's Arthur's. \n",
      " Arthur is still unemployed. Leon sends || him a || him a job offer for\n",
      "========================================\n",
      "Arthur is still unemployed. Leon sends him a job offer for junior project manager position. Arthur is interested. \n",
      " Arthur is still unemployed. Leon sends him a resume to his friend's \n",
      " Arthur is still unemployed. Leon sends him || a || a job offer for\n",
      "========================================\n",
      "Arthur is still unemployed. Leon sends him a job offer for junior project manager position. Arthur is interested. \n",
      " Arthur is still unemployed. Leon sends him a resume to his friend's email. \n",
      " Arthur is still unemployed. Leon sends him a ||  || job offer for\n",
      "========================================\n",
      "Arthur is still unemployed. Leon sends him a job offer for junior project manager position. Arthur is interested. \n",
      " Arthur is still unemployed. Leon sends him a job offer as a junior project manager's \n",
      " Arthur is still unemployed. Leon sends him a job || offer || offer for\n",
      "========================================\n",
      "Arthur is still unemployed. Leon sends him a job offer for junior project manager position. Arthur is interested. \n",
      " Arthur is still unemployed. Leon sends him a job offer as a junior project manager's \n",
      " Arthur is still unemployed. Leon sends him a job offer ||  || for\n",
      "========================================\n",
      "Arthur is still unemployed. Leon sends him a job offer for junior project manager position. Arthur is interested. \n",
      " Arthur is still unemployed. Leon sends him a job offer for junior project manager position. \n",
      " Arthur is still unemployed. Leon sends him a job offer for junior project manager position. ||  || arthur\n",
      "========================================\n",
      "Macca has done ice climbing for the first time today, close to Reykjavik. He enjoyed it very much. \n",
      " Macca has done ice climbing today. He's in Iceland. \n",
      " Macca has done ice climbing ||  || for the first time\n",
      "========================================\n",
      "Macca has done ice climbing for the first time today, close to Reykjavik. He enjoyed it very much. \n",
      " Macca has done ice climbing for the first time today, close to Reykjavik. He enjoyed it. \n",
      " Macca has done ice climbing for the first time today, close to Reykjavik. || he enjoyed || he enjoyed it\n",
      "========================================\n",
      "Macca has done ice climbing for the first time today, close to Reykjavik. He enjoyed it very much. \n",
      " Macca has done ice climbing for the first time today, close to Reykjavik. He enjoyed it. \n",
      " Macca has done ice climbing for the first time today, close to Reykjavik. He || enjoyed || enjoyed it\n",
      "========================================\n",
      "Isabella feels bad after the Christmas party. She got drunk. She is ashamed to go back to work.  \n",
      " Isabella feels bad after the Christmas party. She got drunk. She is ashamed of herself. \n",
      " Isabella feels bad after the Christmas party. She got drunk. She is ashamed ||  || to go back to\n",
      "========================================\n",
      "Lucy owes Tina 50 dollars. She made a transfer but it is Sunday so the payment will be on Tina's account on Monday. Tina needs the money because she has been having expanses recently. \n",
      " Lucy owes Tina 50 dollars. She made a transfer but it is Sunday so the payment will be on Tina's bank account tomorrow \n",
      " Lucy owes Tina 50 dollars. She made a transfer but it is Sunday so the payment will be on Tina's ||  || account\n",
      "========================================\n",
      "Betty feels remorse she got drunk last night and went out of control. \n",
      " Betty feels remorse she got drunk last night. \n",
      " Betty feels remorse she got drunk last night ||  || and\n",
      "========================================\n",
      "Mike and Mary are going to visit Mike's grandma tonight. Mary will buy her some chocolate. \n",
      " Mike and Mary will visit Mike's grandma tonight. Mary will buy some chocolate for her. \n",
      " Mike and || mary || mary are going to visit mike's grandma\n",
      "========================================\n",
      "Laura will pick up Kim from work around 7, and they will come back home together. \n",
      " Laura will pick Kim up at work at 7. \n",
      " Laura will pick ||  || up kim\n",
      "========================================\n",
      "Erin is convinced by Ashley's book recommendations, while Seamus and Marcus aren't. \n",
      " Erin is going to read a book. Ashley is going to \n",
      " Erin is ||  || convinced\n",
      "========================================\n",
      "Erin is convinced by Ashley's book recommendations, while Seamus and Marcus aren't. \n",
      " Erin is convinced that Ashley's book is the best thing she's book is the best. \n",
      " Erin is convinced ||  || by ashley's\n",
      "========================================\n",
      "Aria has just run into Charlie Evans. He is now married, with two daughters, and a family business. She has also met Cooper Roy from high school. She used to have a crush on him, now she almost didn't recognise him. Maverick and Aria miss the old times and think the world has changed for the worse. \n",
      " Aria has just run into Charlie Evans. Charlie Evans got married and has two daughters. \n",
      " Aria has just run into Charlie Evans. ||  || he\n",
      "========================================\n",
      "Aria has just run into Charlie Evans. He is now married, with two daughters, and a family business. She has also met Cooper Roy from high school. She used to have a crush on him, now she almost didn't recognise him. Maverick and Aria miss the old times and think the world has changed for the worse. \n",
      " Aria has just run into Charlie Evans. He is now married and has two daughters. Aria. Aria. Aria has been in a. Aria has been in a. \n",
      " Aria has just run into Charlie Evans. He is now ||  || married,\n",
      "========================================\n",
      "Aria has just run into Charlie Evans. He is now married, with two daughters, and a family business. She has also met Cooper Roy from high school. She used to have a crush on him, now she almost didn't recognise him. Maverick and Aria miss the old times and think the world has changed for the worse. \n",
      " Aria has just run into Charlie Evans. He is now married, with two daughters, and a family business. She has also met Cooper Roy from high school. She had a \n",
      " Aria has just run into Charlie Evans. He is now married, with two daughters, and a family business. She has also met Cooper Roy from high school. She ||  || used to have a crush on\n",
      "========================================\n",
      "Aria has just run into Charlie Evans. He is now married, with two daughters, and a family business. She has also met Cooper Roy from high school. She used to have a crush on him, now she almost didn't recognise him. Maverick and Aria miss the old times and think the world has changed for the worse. \n",
      " Aria has just run into Charlie Evans. He is now married, with two daughters, and a family business. She has also met Cooper Roy from high school. She used to have a crush on him, now she almost didn't recognise him. Maverick miss \n",
      " Aria has just run into Charlie Evans. He is now married, with two daughters, and a family business. She has also met Cooper Roy from high school. She used to have a crush on him, now she almost didn't recognise him. Maverick ||  || and\n",
      "========================================\n",
      "Layla the dog misses Rachel. She is having a knee surgery, because of arthritis. Renee sends a picture of her dog. Rachel getting settled and she's looking for a job in teaching.  \n",
      " Layla the cat is having knee surgery. Renee misse \n",
      " Layla the ||  || dog misses rachel.\n",
      "========================================\n",
      "Layla the dog misses Rachel. She is having a knee surgery, because of arthritis. Renee sends a picture of her dog. Rachel getting settled and she's looking for a job in teaching.  \n",
      " Layla the dog needs knee surgery. Renee misses Rachel is having surgery. \n",
      " Layla the dog ||  || misses rachel.\n",
      "========================================\n",
      "Layla the dog misses Rachel. She is having a knee surgery, because of arthritis. Renee sends a picture of her dog. Rachel getting settled and she's looking for a job in teaching.  \n",
      " Layla the dog misses Renee. Layla is having knee surgery. \n",
      " Layla the dog misses ||  || rachel.\n",
      "========================================\n",
      "Layla the dog misses Rachel. She is having a knee surgery, because of arthritis. Renee sends a picture of her dog. Rachel getting settled and she's looking for a job in teaching.  \n",
      " Layla the dog misses Rachel. She is having a knee surgery, because of arthritis. Renee sends her a \n",
      " Layla the dog misses Rachel. She is having a knee surgery, because of arthritis. Renee sends ||  || a picture\n",
      "========================================\n",
      "Layla the dog misses Rachel. She is having a knee surgery, because of arthritis. Renee sends a picture of her dog. Rachel getting settled and she's looking for a job in teaching.  \n",
      " Layla the dog misses Rachel. She is having a knee surgery, because of arthritis. Renee sends a picture of her dog. Rachel getting settled and she's looking for a job with \n",
      " Layla the dog misses Rachel. She is having a knee surgery, because of arthritis. Renee sends a picture of her dog. Rachel getting settled and she's looking for a || job || job in\n",
      "========================================\n",
      "Jonas will be 10 minutes late. Natalie will let Mary know Jonas will present before she starts. \n",
      " Jonas is 10 minutes late for Mary's presentation. Natalie and Olivia will let Mary. \n",
      " Jonas ||  || will be 10 minutes\n",
      "========================================\n",
      "Jonas will be 10 minutes late. Natalie will let Mary know Jonas will present before she starts. \n",
      " Jonas will be 10 minutes late. Natalie will let Mary know that Jonas will let Mary will present. \n",
      " Jonas will be 10 minutes late. Natalie will || let mary know || let mary know jonas will\n",
      "========================================\n",
      "Jonas will be 10 minutes late. Natalie will let Mary know Jonas will present before she starts. \n",
      " Jonas will be 10 minutes late. Natalie will let Mary know that Jonas will present today. \n",
      " Jonas will be 10 minutes late. Natalie will let || mary know || mary know jonas will present\n",
      "========================================\n",
      "Jonas will be 10 minutes late. Natalie will let Mary know Jonas will present before she starts. \n",
      " Jonas will be 10 minutes late. Natalie will let Mary know that Jonas will be 10 minutes before she will present. \n",
      " Jonas will be 10 minutes late. Natalie will let Mary || know || know jonas will present\n",
      "========================================\n",
      "Jonas will be 10 minutes late. Natalie will let Mary know Jonas will present before she starts. \n",
      " Jonas will be 10 minutes late. Natalie will let Mary know that Jonas will presenter \n",
      " Jonas will be 10 minutes late. Natalie will let Mary know ||  || jonas will present before she\n",
      "========================================\n",
      "Jonas will be 10 minutes late. Natalie will let Mary know Jonas will present before she starts. \n",
      " Jonas will be 10 minutes late. Natalie will let Mary know Jonas will present today before she will be 10 minutes before she starts. \n",
      " Jonas will be 10 minutes late. Natalie will let Mary know Jonas || will present || will present before she\n",
      "========================================\n",
      "Jonas will be 10 minutes late. Natalie will let Mary know Jonas will present before she starts. \n",
      " Jonas will be 10 minutes late. Natalie will let Mary know Jonas will present today before she starts. \n",
      " Jonas will be 10 minutes late. Natalie will let Mary know Jonas will || present || present before she starts.\n",
      "========================================\n",
      "Jonas will be 10 minutes late. Natalie will let Mary know Jonas will present before she starts. \n",
      " Jonas will be 10 minutes late. Natalie will let Mary know Jonas will present today before she starts. \n",
      " Jonas will be 10 minutes late. Natalie will let Mary know Jonas will present ||  || before she starts.\n",
      "========================================\n",
      "Lawrence doesn't like the play of Manchester United. He and Julius complain about the team and Mourinho's style. \n",
      " Lawrence doesn't like the play of Manchester United. Julius and Lawrence think the team. \n",
      " Lawrence doesn't like the play of Manchester United. ||  || he\n",
      "========================================\n",
      "The trip Wayne was going to go on was postponed. Now Jade will be able to go to. \n",
      " The trip Wayne was going to go on has been postponed. Jade had to. \n",
      " The trip Wayne was going to go || on || on was postponed.\n",
      "========================================\n",
      "The trip Wayne was going to go on was postponed. Now Jade will be able to go to. \n",
      " The trip Wayne was going to go on has been postponed. Jade had to. \n",
      " The trip Wayne was going to go on ||  || was postponed.\n",
      "========================================\n",
      "Jason is going to Thailand next week. Natalie will send him pictures of the spices she wants him to buy for her. Then she will remind him about it in two weeks. \n",
      " Jason is going to Thailand next week. Natalie will send him all the names of the spices. \n",
      " Jason is going to Thailand next week. Natalie will send him ||  || pictures of\n",
      "========================================\n",
      "Elisa, Sadie, Carol, Liam, Tom and John want to go for drinks tonight at Mombasa. Arthur will come with Alice. Kai will join them around 8. Elisa will book the big corner table for 15 people. \n",
      " Elisa, Sadie, Carol, Liam, Tom and John want to go for drinks tonight at Mombasa. Arthur will come with Alice. Kai will join Elisa. \n",
      " Elisa, Sadie, Carol, Liam, Tom and John want to go for drinks tonight at Mombasa. Arthur will come with Alice. Kai will || join || join them\n",
      "========================================\n",
      "Elisa, Sadie, Carol, Liam, Tom and John want to go for drinks tonight at Mombasa. Arthur will come with Alice. Kai will join them around 8. Elisa will book the big corner table for 15 people. \n",
      " Elisa, Sadie, Carol, Liam, Tom and John want to go for drinks tonight at Mombasa. Arthur will come with Alice. Kai will join them around 8. Elisa will book the big table for 15 people. \n",
      " Elisa, Sadie, Carol, Liam, Tom and John want to go for drinks tonight at Mombasa. Arthur will come with Alice. Kai will join them around 8. Elisa will || book the big || book the big corner table for\n",
      "========================================\n",
      "Amy and Hal will have dinner together when he is back home before 7. \n",
      " Amy and Hal will have dinner together when he is home around 6. \n",
      " Amy and Hal will have dinner together when he is ||  || back home before 7.\n",
      "========================================\n",
      "Amy and Hal will have dinner together when he is back home before 7. \n",
      " Amy and Hal will have dinner together when he is back home at 7. \n",
      " Amy and Hal will have dinner together when he is back home ||  || before 7.\n",
      "========================================\n",
      "Eric wants to bet during the Champions League this year although he lost a lot of money last year. Curtis won't bet. \n",
      " Eric wants to bet during the Champions League this year although he has a 50% chance to win \n",
      " Eric wants to bet during the Champions League this year although || he || he lost a\n",
      "========================================\n",
      "Eric wants to bet during the Champions League this year although he lost a lot of money last year. Curtis won't bet. \n",
      " Eric wants to bet during the Champions League this year although he lost a lot. Curtis 50% chance to win. \n",
      " Eric wants to bet during the Champions League this year although he || lost a || lost a lot of\n",
      "========================================\n",
      "Eric wants to bet during the Champions League this year although he lost a lot of money last year. Curtis won't bet. \n",
      " Eric wants to bet during the Champions League this year although he lost a lot. Curtise. \n",
      " Eric wants to bet during the Champions League this year although he lost || a || a lot of money last year.\n",
      "========================================\n",
      "Eric wants to bet during the Champions League this year although he lost a lot of money last year. Curtis won't bet. \n",
      " Eric wants to bet during the Champions League this year although he lost a lot. Curtis \n",
      " Eric wants to bet during the Champions League this year although he lost a ||  || lot of money last\n",
      "========================================\n",
      "Chandler will pay for his coffee tomorrow. \n",
      " Chandler didn't pay for coffee. Chandler will pay him tomorrow. \n",
      " Chandler ||  || will pay for\n",
      "========================================\n",
      "Ted, Jake, Pia, Jessica and Jess are having a reunion this Friday at the same place as the previous one. \n",
      " Ted, Jake, Pia, Jessica and Jake are meeting for a, Jessica and Jessica will meet on Friday night at the place they did last time. \n",
      " Ted, Jake, || pia, jessica and || pia, jessica and jess are\n",
      "========================================\n",
      "Ted, Jake, Pia, Jessica and Jess are having a reunion this Friday at the same place as the previous one. \n",
      " Ted, Jake, Pia, Jessica and Jake are meeting for a reunion is going to meet on Friday night at the place they did last time. \n",
      " Ted, Jake, Pia, || jessica and || jessica and jess are\n",
      "========================================\n",
      "Ted, Jake, Pia, Jessica and Jess are having a reunion this Friday at the same place as the previous one. \n",
      " Ted, Jake, Pia, Jessica and Ted are meeting on Friday night at the reunion is going to meet up at the place they did last time. \n",
      " Ted, Jake, Pia, Jessica || and || and jess are\n",
      "========================================\n",
      "Ted, Jake, Pia, Jessica and Jess are having a reunion this Friday at the same place as the previous one. \n",
      " Ted, Jake, Pia, Jessica and Pia are meeting on Friday night at the group will meet at the place they did last time. \n",
      " Ted, Jake, Pia, Jessica and ||  || jess are\n",
      "========================================\n",
      "Ted, Jake, Pia, Jessica and Jess are having a reunion this Friday at the same place as the previous one. \n",
      " Ted, Jake, Pia, Jessica and Jess are having a reunion this Friday at the same place they met last time. \n",
      " Ted, Jake, Pia, Jessica and Jess are having a reunion this Friday at the same || place || place as\n",
      "========================================\n",
      "Julianna tells Bradley about Europe's law and the approach towards pedophiles. \n",
      " Julianna tells Bradley about Europe's law and the approach of teachers. \n",
      " Julianna tells Bradley about Europe's law and the approach ||  || towards\n",
      "========================================\n",
      "Lucia needs a new hairstyle due to a change of work and she makes an appointment with Eric for Friday 3 p.m. \n",
      " Lucia needs a new hairstyle due to a change of work and she makes an appointment with Eric on Friday at 3 a. \n",
      " Lucia needs a new hairstyle due to a change of work and she makes an appointment with Eric ||  || for friday\n",
      "========================================\n",
      "Gabriella asked Jasmine to check her CV which is in English. She's applying for a perfect job. Jasmine checked it and did only minor corrections. She hopes Gabriella will get the job. \n",
      " Gabriella asked Jasmine to check her CV which is in English. She's applying for a perfect job. Jasmine will \n",
      " Gabriella asked Jasmine to check her CV which is in English. She's applying for a perfect job. || jasmine || jasmine checked\n",
      "========================================\n",
      "Gabriella asked Jasmine to check her CV which is in English. She's applying for a perfect job. Jasmine checked it and did only minor corrections. She hopes Gabriella will get the job. \n",
      " Gabriella asked Jasmine to check her CV which is in English. She's applying for a perfect job. Jasmine fixed a \n",
      " Gabriella asked Jasmine to check her CV which is in English. She's applying for a perfect job. Jasmine ||  || checked it and\n",
      "========================================\n",
      "Grace saw Ruth in Galitos but she thought it would be embarassing to get in and greet her. They will meet tomorrow. \n",
      " Grace saw Ruth in Galitos but didn't stop her. Ruth will meet with her. \n",
      " Grace saw Ruth in Galitos but ||  || she\n",
      "========================================\n",
      "Grace saw Ruth in Galitos but she thought it would be embarassing to get in and greet her. They will meet tomorrow. \n",
      " Grace saw Ruth in Galitos but she thought she should have stopped her. Ruth was too busy. \n",
      " Grace saw Ruth in Galitos but she thought ||  || it\n",
      "========================================\n",
      "Grace saw Ruth in Galitos but she thought it would be embarassing to get in and greet her. They will meet tomorrow. \n",
      " Grace saw Ruth in Galitos but she thought it would be better to stop her. \n",
      " Grace saw Ruth in Galitos but she thought it would || be || be embarassing\n",
      "========================================\n",
      "Grace saw Ruth in Galitos but she thought it would be embarassing to get in and greet her. They will meet tomorrow. \n",
      " Grace saw Ruth in Galitos but she thought it would be better to stop her. Ruth would like to meet tomorrow. \n",
      " Grace saw Ruth in Galitos but she thought it would be ||  || embarassing to\n",
      "========================================\n",
      "Grace saw Ruth in Galitos but she thought it would be embarassing to get in and greet her. They will meet tomorrow. \n",
      " Grace saw Ruth in Galitos but she thought it would be embarassing not to greet her. \n",
      " Grace saw Ruth in Galitos but she thought it would be embarassing ||  || to\n",
      "========================================\n",
      "Grace saw Ruth in Galitos but she thought it would be embarassing to get in and greet her. They will meet tomorrow. \n",
      " Grace saw Ruth in Galitos but she thought it would be embarassing to get to know her. \n",
      " Grace saw Ruth in Galitos but she thought it would be embarassing to get ||  || in and\n",
      "========================================\n",
      "Grace saw Ruth in Galitos but she thought it would be embarassing to get in and greet her. They will meet tomorrow. \n",
      " Grace saw Ruth in Galitos but she thought it would be embarassing to get in touch with her. \n",
      " Grace saw Ruth in Galitos but she thought it would be embarassing to get in ||  || and\n",
      "========================================\n",
      "Stella wants to see the apartment in the morning. She plans to split it into 2 apartments. \n",
      " Stella wants to see the apartment. It's 130 sqm \n",
      " Stella wants || to see the || to see the apartment in the morning.\n",
      "========================================\n",
      "Stella wants to see the apartment in the morning. She plans to split it into 2 apartments. \n",
      " Stella wants to see the apartment. It's 130 sqm. \n",
      " Stella wants to || see the || see the apartment in the morning.\n",
      "========================================\n",
      "Stella wants to see the apartment in the morning. She plans to split it into 2 apartments. \n",
      " Stella wants to see the 130 sqm apartment in the morning. Stella \n",
      " Stella wants to see || the || the apartment in the morning.\n",
      "========================================\n",
      "Stella wants to see the apartment in the morning. She plans to split it into 2 apartments. \n",
      " Stella wants to see the 130 sqm apartment in the morning. Stella \n",
      " Stella wants to see the ||  || apartment in the morning.\n",
      "========================================\n",
      "Andrea must correct 50% of 20 short texts for an online shop. She has a deadline in two weeks. Sondra cannot help Andrea, because her cat is dying and nanny's leaving. Jill also cannot help. Sondra will probably have only one free evening in March and more free time probably in June.  \n",
      " Andrea must write 20 short texts for an online shop. Sondra's for an online shop. \n",
      " Andrea must ||  || correct 50% of\n",
      "========================================\n",
      "Andrea must correct 50% of 20 short texts for an online shop. She has a deadline in two weeks. Sondra cannot help Andrea, because her cat is dying and nanny's leaving. Jill also cannot help. Sondra will probably have only one free evening in March and more free time probably in June.  \n",
      " Andrea must correct 20 short texts for an online shop. Sondra's texts for an online shop. \n",
      " Andrea must correct ||  || 50% of\n",
      "========================================\n",
      "Andrea must correct 50% of 20 short texts for an online shop. She has a deadline in two weeks. Sondra cannot help Andrea, because her cat is dying and nanny's leaving. Jill also cannot help. Sondra will probably have only one free evening in March and more free time probably in June.  \n",
      " Andrea must correct 50% of 20 short texts for an online shop. She has a deadline in two weeks. Sondra cannot help Andrea, because her cat is dying and nanny's leaving. Jill also cannot help. Sondra will probably have only one free evening in March and more in \n",
      " Andrea must correct 50% of 20 short texts for an online shop. She has a deadline in two weeks. Sondra cannot help Andrea, because her cat is dying and nanny's leaving. Jill also cannot help. Sondra will probably have only one free evening in March and more ||  || free\n",
      "========================================\n",
      "Andrea must correct 50% of 20 short texts for an online shop. She has a deadline in two weeks. Sondra cannot help Andrea, because her cat is dying and nanny's leaving. Jill also cannot help. Sondra will probably have only one free evening in March and more free time probably in June.  \n",
      " Andrea must correct 50% of 20 short texts for an online shop. She has a deadline in two weeks. Sondra cannot help Andrea, because her cat is dying and nanny's leaving. Jill also cannot help. Sondra will probably have only one free evening in March and more free evening \n",
      " Andrea must correct 50% of 20 short texts for an online shop. She has a deadline in two weeks. Sondra cannot help Andrea, because her cat is dying and nanny's leaving. Jill also cannot help. Sondra will probably have only one free evening in March and more free ||  || time\n",
      "========================================\n",
      "Elizabeth had an argument with Blackett today, but she doesn't want William to intermeddle. \n",
      " Elizabeth had an argument with Blackett today, but they will solve it. \n",
      " Elizabeth had an argument with Blackett today, || but || but she\n",
      "========================================\n",
      "Elizabeth had an argument with Blackett today, but she doesn't want William to intermeddle. \n",
      " Elizabeth had an argument with Blackett today, but it was just a small. \n",
      " Elizabeth had an argument with Blackett today, but ||  || she\n",
      "========================================\n",
      "Elizabeth had an argument with Blackett today, but she doesn't want William to intermeddle. \n",
      " Elizabeth had an argument with Blackett today, but she will solve it. William will call her. \n",
      " Elizabeth had an argument with Blackett today, but she ||  || doesn't want william to\n",
      "========================================\n",
      "Elizabeth had an argument with Blackett today, but she doesn't want William to intermeddle. \n",
      " Elizabeth had an argument with Blackett today, but she doesn't want to talk to him \n",
      " Elizabeth had an argument with Blackett today, but she doesn't || want || want william to\n",
      "========================================\n",
      "Elizabeth had an argument with Blackett today, but she doesn't want William to intermeddle. \n",
      " Elizabeth had an argument with Blackett today, but she doesn't want to talk to him about something to talk to \n",
      " Elizabeth had an argument with Blackett today, but she doesn't want ||  || william to\n",
      "========================================\n",
      "Henry and Lily take William with them to Riehen. They will meet with him for a coffee before they start, and eat a meal together afterwards. \n",
      " Henry and Lily take William with them to Riehen next week. They will meet for a coffee at Henry. \n",
      " Henry and Lily take William with || them to || them to riehen. they will meet\n",
      "========================================\n",
      "Henry and Lily take William with them to Riehen. They will meet with him for a coffee before they start, and eat a meal together afterwards. \n",
      " Henry and Lily take William with them to Riehen next week. Lily to a pub. \n",
      " Henry and Lily take William with them || to || to riehen. they will meet\n",
      "========================================\n",
      "Henry and Lily take William with them to Riehen. They will meet with him for a coffee before they start, and eat a meal together afterwards. \n",
      " Henry and Lily take William with them to Riehen next week. Lily to a pub. \n",
      " Henry and Lily take William with them to ||  || riehen. they will meet\n",
      "========================================\n",
      "Henry and Lily take William with them to Riehen. They will meet with him for a coffee before they start, and eat a meal together afterwards. \n",
      " Henry and Lily take William with them to Riehen. They will have a coffee at Henry will bring William to a meal together. \n",
      " Henry and Lily take William with them to Riehen. || they will || they will meet\n",
      "========================================\n",
      "Henry and Lily take William with them to Riehen. They will meet with him for a coffee before they start, and eat a meal together afterwards. \n",
      " Henry and Lily take William with them to Riehen. They will have a coffee at Henry will bring William to a meal together. \n",
      " Henry and Lily take William with them to Riehen. They || will || will meet\n",
      "========================================\n",
      "Henry and Lily take William with them to Riehen. They will meet with him for a coffee before they start, and eat a meal together afterwards. \n",
      " Henry and Lily take William with them to Riehen. They will have a coffee at Henry will bring William to a meal together. \n",
      " Henry and Lily take William with them to Riehen. They will ||  || meet\n",
      "========================================\n",
      "Henry and Lily take William with them to Riehen. They will meet with him for a coffee before they start, and eat a meal together afterwards. \n",
      " Henry and Lily take William with them to Riehen. They will meet with him for a coffee before they start, and eat a meal. \n",
      " Henry and Lily take William with them to Riehen. They will meet with him for a coffee before they start, and eat || a || a meal\n",
      "========================================\n",
      "According to Brian, colors that match Linda's personality are yellow and grey. \n",
      " According to Brian, Linda's room will be painted. Linda is going to the colors will be yellow and grey and she will be painted. \n",
      " According || to brian, || to brian, colors that\n",
      "========================================\n",
      "According to Brian, colors that match Linda's personality are yellow and grey. \n",
      " According to Brian, Linda's room will be painted. Linda is going to the colors will be yellow and grey and she will be painted. \n",
      " According to || brian, || brian, colors that\n",
      "========================================\n",
      "According to Brian, colors that match Linda's personality are yellow and grey. \n",
      " According to Brian, colors for Linda's personality are yellow and grey. \n",
      " According to Brian, colors ||  || that\n",
      "========================================\n",
      "Jimmy is going to take medication for a month to cure his acute gastritis. \n",
      " Jimmy is going to see a doctor. He has acute gastritis doctor. \n",
      " Jimmy is going to ||  || take\n",
      "========================================\n",
      "Jimmy is going to take medication for a month to cure his acute gastritis. \n",
      " Jimmy is going to take medication for acute gastritis. \n",
      " Jimmy is going to take medication for a ||  || month to cure his acute\n",
      "========================================\n",
      "Jimmy is going to take medication for a month to cure his acute gastritis. \n",
      " Jimmy is going to take medication for a month to treat acute gastritis \n",
      " Jimmy is going to take medication for a month || to || to cure his acute\n",
      "========================================\n",
      "Jimmy is going to take medication for a month to cure his acute gastritis. \n",
      " Jimmy is going to take medication for a month to treat acute gastritis. \n",
      " Jimmy is going to take medication for a month to ||  || cure his acute\n",
      "========================================\n",
      "Jimmy is going to take medication for a month to cure his acute gastritis. \n",
      " Jimmy is going to take medication for a month to cure acute gastritis. \n",
      " Jimmy is going to take medication for a month to cure ||  || his acute gastritis.\n",
      "========================================\n",
      "Peter will be back home in 2 days. Mary would like to go on a date when he gets back. \n",
      " Peter will be back home in 2 days and he and Mary will meet up. \n",
      " Peter || will be back home in 2 || will be back home in 2 days.\n",
      "========================================\n",
      "Peter will be back home in 2 days. Mary would like to go on a date when he gets back. \n",
      " Peter will be back home in 2 days and he and Mary will meet up. \n",
      " Peter will || be back home in 2 || be back home in 2 days.\n",
      "========================================\n",
      "Peter will be back home in 2 days. Mary would like to go on a date when he gets back. \n",
      " Peter will be back home in 2 days and he and Mary will meet up. \n",
      " Peter will be || back home in 2 || back home in 2 days.\n",
      "========================================\n",
      "Neither Francesca nor Jacob can sleep. Jacob has suicidal thoughts.  \n",
      " Neither Francesca nor Jacob can sleep. Jacob is sad about Francesca's's'sca's's's's's's's mum. \n",
      " Neither Francesca nor Jacob can sleep. Jacob ||  || has\n",
      "========================================\n",
      "Julie recommends the GlamRock cream from the GlamShop. Chloe got the money early so she can pay back Julie. Julie and Jake are getting the dog so they need the money for extra dog-related expenses. \n",
      " Julie recommends the GlamRock cream from the GlamShop to Chloed shop to Chloe. Julie and Jake and Julie are looking for the. \n",
      " Julie recommends the GlamRock cream from the ||  || glamshop.\n",
      "========================================\n",
      "Julie recommends the GlamRock cream from the GlamShop. Chloe got the money early so she can pay back Julie. Julie and Jake are getting the dog so they need the money for extra dog-related expenses. \n",
      " Julie recommends the GlamRock cream from the GlamShop. Chloe borrowed money from Julie. \n",
      " Julie recommends the GlamRock cream from the GlamShop. Chloe ||  || got\n",
      "========================================\n",
      "Julie recommends the GlamRock cream from the GlamShop. Chloe got the money early so she can pay back Julie. Julie and Jake are getting the dog so they need the money for extra dog-related expenses. \n",
      " Julie recommends the GlamRock cream from the GlamShop. Chloe got money earlier than expected. Julie and Jake are looking for the cream. \n",
      " Julie recommends the GlamRock cream from the GlamShop. Chloe got ||  || the money\n",
      "========================================\n",
      "Julie recommends the GlamRock cream from the GlamShop. Chloe got the money early so she can pay back Julie. Julie and Jake are getting the dog so they need the money for extra dog-related expenses. \n",
      " Julie recommends the GlamRock cream from the GlamShop. Chloe got the money early so she can buy a dog. Julie and Jake want to \n",
      " Julie recommends the GlamRock cream from the GlamShop. Chloe got the money early so she can ||  || pay back\n",
      "========================================\n",
      "Julie recommends the GlamRock cream from the GlamShop. Chloe got the money early so she can pay back Julie. Julie and Jake are getting the dog so they need the money for extra dog-related expenses. \n",
      " Julie recommends the GlamRock cream from the GlamShop. Chloe got the money early so she can pay back Julie. Julie \n",
      " Julie recommends the GlamRock cream from the GlamShop. Chloe got the money early so she can pay back || julie. julie || julie. julie and\n",
      "========================================\n",
      "Teddy has a message from Pitt on Messenger. \n",
      " Teddy hasn't received Pitt's message. \n",
      " Teddy ||  || has\n",
      "========================================\n",
      "Mary didn't come to Nick's birthday party. She met an architect named Kirk. Mary and Nick will meet in the evening. \n",
      " Mary didn't come to Nick's birthday party. She met an architect named Kirk and they spent the whole week. \n",
      " Mary didn't come to Nick's birthday party. She met an architect || named || named kirk.\n",
      "========================================\n",
      "Mary didn't come to Nick's birthday party. She met an architect named Kirk. Mary and Nick will meet in the evening. \n",
      " Mary didn't come to Nick's birthday party. She met an architect named Kirk and they spent a's \n",
      " Mary didn't come to Nick's birthday party. She met an architect named ||  || kirk.\n",
      "========================================\n",
      "Mary didn't come to Nick's birthday party. She met an architect named Kirk. Mary and Nick will meet in the evening. \n",
      " Mary didn't come to Nick's birthday party. She met an architect named Kirk. Mary and Kirk spent  \n",
      " Mary didn't come to Nick's birthday party. She met an architect named Kirk. Mary || and || and nick\n",
      "========================================\n",
      "Mary didn't come to Nick's birthday party. She met an architect named Kirk. Mary and Nick will meet in the evening. \n",
      " Mary didn't come to Nick's birthday party. She met an architect named Kirk. Mary and Kirk spent a \n",
      " Mary didn't come to Nick's birthday party. She met an architect named Kirk. Mary and ||  || nick\n",
      "========================================\n",
      "Mary didn't come to Nick's birthday party. She met an architect named Kirk. Mary and Nick will meet in the evening. \n",
      " Mary didn't come to Nick's birthday party. She met an architect named Kirk. Mary and Nick spent a week together. \n",
      " Mary didn't come to Nick's birthday party. She met an architect named Kirk. Mary and Nick ||  || will meet\n",
      "========================================\n",
      "Mary didn't come to Nick's birthday party. She met an architect named Kirk. Mary and Nick will meet in the evening. \n",
      " Mary didn't come to Nick's birthday party. She met an architect named Kirk. Mary and Nick will talk this evening. \n",
      " Mary didn't come to Nick's birthday party. She met an architect named Kirk. Mary and Nick will ||  || meet\n",
      "========================================\n",
      "Connor will not attend the meeting, he has to rest. Isaiah never wanted to go. \n",
      " Connor will not attend the meeting, he has to rest. Isaiah never wanted to go to the meeting. \n",
      " Connor will not attend the meeting, he has to rest. Isaiah never wanted || to || to go.\n",
      "========================================\n",
      "Samara bought the dress she had at the wedding in an online shop and she was happy when she discovered it. \n",
      " Samara bought the dress she had at the wedding in an online shop and she'll probably become as she's a more often. \n",
      " Samara bought the dress she had at the wedding in an online shop and ||  || she was happy when she discovered it.\n",
      "========================================\n",
      "Samara bought the dress she had at the wedding in an online shop and she was happy when she discovered it. \n",
      " Samara bought the dress she had at the wedding in an online shop and she'll probably become  \n",
      " Samara bought the dress she had at the wedding in an online shop and she ||  || was happy when she discovered\n",
      "========================================\n",
      "Samara bought the dress she had at the wedding in an online shop and she was happy when she discovered it. \n",
      " Samara bought the dress she had at the wedding in an online shop and she was happy about it. \n",
      " Samara bought the dress she had at the wedding in an online shop and she was || happy || happy when she discovered it.\n",
      "========================================\n",
      "Samara bought the dress she had at the wedding in an online shop and she was happy when she discovered it. \n",
      " Samara bought the dress she had at the wedding in an online shop and she was happy with it. She's. \n",
      " Samara bought the dress she had at the wedding in an online shop and she was happy ||  || when she discovered it.\n",
      "========================================\n",
      "Mary, Kate, Peter and John want various items for Christmas. Gabi wants a surprise. Jack is unsure what he wants. Jack should let the rest know within a week.  \n",
      " Mary, Kate, Peter and John want various items for Christmas. Gabi wants a surprise. Jack is unsure what he wants. Jack should let the group know in about it in about \n",
      " Mary, Kate, Peter and John want various items for Christmas. Gabi wants a surprise. Jack is unsure what he wants. Jack should let the ||  || rest know\n",
      "========================================\n",
      "Mary, Kate, Peter and John want various items for Christmas. Gabi wants a surprise. Jack is unsure what he wants. Jack should let the rest know within a week.  \n",
      " Mary, Kate, Peter and John want various items for Christmas. Gabi wants a surprise. Jack is unsure what he wants. Jack should let the rest of the group in about it in about it in about it. \n",
      " Mary, Kate, Peter and John want various items for Christmas. Gabi wants a surprise. Jack is unsure what he wants. Jack should let the rest ||  || know\n",
      "========================================\n",
      "Jeff will double check Mindy's reaction, connected to her involvement in phase one of the project. \n",
      " Jeff will check if Mindy was there. \n",
      " Jeff will ||  || double check\n",
      "========================================\n",
      "Client informs Flix about a 40 minutes delay of the bus from Radom to Warsaw. Flix explains that this is an operational delay. Client decides to make a formal complaint as it wasn't the first such delay. \n",
      " Client informs Flix about a 40-minute delay on the P6 bus from Cracowl \n",
      " Client informs || flix about a || flix about a 40 minutes delay\n",
      "========================================\n",
      "Client informs Flix about a 40 minutes delay of the bus from Radom to Warsaw. Flix explains that this is an operational delay. Client decides to make a formal complaint as it wasn't the first such delay. \n",
      " Client informs Flix that the P6 bus Radom - Warsaw-Bus is late because of the company about the company about the company about the company about the bus delays. \n",
      " Client informs Flix ||  || about a 40 minutes delay\n",
      "========================================\n",
      "Client informs Flix about a 40 minutes delay of the bus from Radom to Warsaw. Flix explains that this is an operational delay. Client decides to make a formal complaint as it wasn't the first such delay. \n",
      " Client informs Flix about a 40-minute delay on the P6 bus from Cracowl \n",
      " Client informs Flix about || a || a 40 minutes delay\n",
      "========================================\n",
      "Client informs Flix about a 40 minutes delay of the bus from Radom to Warsaw. Flix explains that this is an operational delay. Client decides to make a formal complaint as it wasn't the first such delay. \n",
      " Client informs Flix about a 40-minute delay on the P6 bus from Cracowl \n",
      " Client informs Flix about a ||  || 40 minutes delay\n",
      "========================================\n",
      "Client informs Flix about a 40 minutes delay of the bus from Radom to Warsaw. Flix explains that this is an operational delay. Client decides to make a formal complaint as it wasn't the first such delay. \n",
      " Client informs Flix about a 40 minutes delay of the bus Radom - Warsaw-ed about the bus from Cracow \n",
      " Client informs Flix about a 40 minutes delay of the bus ||  || from\n",
      "========================================\n",
      "Client informs Flix about a 40 minutes delay of the bus from Radom to Warsaw. Flix explains that this is an operational delay. Client decides to make a formal complaint as it wasn't the first such delay. \n",
      " Client informs Flix about a 40 minutes delay of the bus from Cracow to Warsaw to the timetable. \n",
      " Client informs Flix about a 40 minutes delay of the bus from ||  || radom to warsaw.\n",
      "========================================\n",
      "Client informs Flix about a 40 minutes delay of the bus from Radom to Warsaw. Flix explains that this is an operational delay. Client decides to make a formal complaint as it wasn't the first such delay. \n",
      " Client informs Flix about a 40 minutes delay of the bus from Radom to Warsaw. Flix explains that this is an operational delay and will informs about the bus. \n",
      " Client informs Flix about a 40 minutes delay of the bus from Radom to Warsaw. Flix explains that this is an || operational || operational delay.\n",
      "========================================\n",
      "Client informs Flix about a 40 minutes delay of the bus from Radom to Warsaw. Flix explains that this is an operational delay. Client decides to make a formal complaint as it wasn't the first such delay. \n",
      " Client informs Flix about a 40 minutes delay of the bus from Radom to Warsaw. Flix explains that this is an operational delay. Client decides to make a formal complaint as he \n",
      " Client informs Flix about a 40 minutes delay of the bus from Radom to Warsaw. Flix explains that this is an operational delay. Client decides to make a formal complaint as ||  || it\n",
      "========================================\n",
      "Jeff, Ann, Corina, and Maria are going to hike tomorrow. Jeff reminds everyone to take good shoes, because they are going to walk over 20 km. Jeff, Ann, Corina, and Maria will take a bus on the way back at 5 PM. \n",
      " Jeff, Ann, Corina, and Maria are going to hike tomorrow. Jeff reminds everyone to take good shoes, because they are going to take the track is 21. \n",
      " Jeff, Ann, Corina, and Maria are going to hike tomorrow. Jeff reminds everyone to take good shoes, because they are || going to || going to walk\n",
      "========================================\n",
      "Jeff, Ann, Corina, and Maria are going to hike tomorrow. Jeff reminds everyone to take good shoes, because they are going to walk over 20 km. Jeff, Ann, Corina, and Maria will take a bus on the way back at 5 PM. \n",
      " Jeff, Ann, Corina, and Maria are going to hike tomorrow. Jeff reminds everyone to take good shoes, because they are going to walk over 20 km. Jeff, Ann and \n",
      " Jeff, Ann, Corina, and Maria are going to hike tomorrow. Jeff reminds everyone to take good shoes, because they are going to walk over 20 km. Jeff, ||  || ann,\n",
      "========================================\n",
      "Jeff, Ann, Corina, and Maria are going to hike tomorrow. Jeff reminds everyone to take good shoes, because they are going to walk over 20 km. Jeff, Ann, Corina, and Maria will take a bus on the way back at 5 PM. \n",
      " Jeff, Ann, Corina, and Maria are going to hike tomorrow. Jeff reminds everyone to take good shoes, because they are going to walk over 20 km. Jeff, Ann, Corina, and Maria will take a bus on the way back at 5 pm \n",
      " Jeff, Ann, Corina, and Maria are going to hike tomorrow. Jeff reminds everyone to take good shoes, because they are going to walk over 20 km. Jeff, Ann, Corina, and Maria will take a bus on the way || back at 5 || back at 5 pm.\n",
      "========================================\n",
      "Mia hasn't subscribed to Aiden's channel yet but she wants to. Aiden has been working on it for 6 months. \n",
      " Mia hasn't subscribed to Aiden's channel yet but she wants to. Aiden has been working on it's \n",
      " Mia hasn't subscribed to Aiden's channel yet but she wants to. || aiden has been working on || aiden has been working on it for 6 months.\n",
      "========================================\n",
      "Mia hasn't subscribed to Aiden's channel yet but she wants to. Aiden has been working on it for 6 months. \n",
      " Mia hasn't subscribed to Aiden's channel yet but she wants to. Aiden has been working on it's for 6 months. \n",
      " Mia hasn't subscribed to Aiden's channel yet but she wants to. Aiden has || been working on || been working on it for 6 months.\n",
      "========================================\n",
      "Blake believes that Alexis is trying to turn their son, Steven, against him and has bad influence on Steven. Alexis believes Blake is self-centered and often attacks her and accuses her of things she didn't do.   \n",
      " Blake doesn't want to talk to Alexis about his son Steven's to talk to Steven. \n",
      " Blake ||  || believes\n",
      "========================================\n",
      "Blake believes that Alexis is trying to turn their son, Steven, against him and has bad influence on Steven. Alexis believes Blake is self-centered and often attacks her and accuses her of things she didn't do.   \n",
      " Blake believes that Alexis is trying to turn their son, Steven, against him and has bad influence on Steven. Alexis believes Blake is self-centered. Alexis. \n",
      " Blake believes that Alexis is trying to turn their son, Steven, against him and has bad influence on Steven. Alexis believes Blake is self-centered ||  || and\n",
      "========================================\n",
      "Blake believes that Alexis is trying to turn their son, Steven, against him and has bad influence on Steven. Alexis believes Blake is self-centered and often attacks her and accuses her of things she didn't do.   \n",
      " Blake believes that Alexis is trying to turn their son, Steven, against him and has bad influence on Steven. Alexis believes Blake is self-centered and often insults her. \n",
      " Blake believes that Alexis is trying to turn their son, Steven, against him and has bad influence on Steven. Alexis believes Blake is self-centered and often ||  || attacks\n",
      "========================================\n",
      "Blake believes that Alexis is trying to turn their son, Steven, against him and has bad influence on Steven. Alexis believes Blake is self-centered and often attacks her and accuses her of things she didn't do.   \n",
      " Blake believes that Alexis is trying to turn their son, Steven, against him and has bad influence on Steven. Alexis believes Blake is self-centered and often attacks her and accuses her of being \n",
      " Blake believes that Alexis is trying to turn their son, Steven, against him and has bad influence on Steven. Alexis believes Blake is self-centered and often attacks her and accuses her || of || of things\n",
      "========================================\n",
      "Blake believes that Alexis is trying to turn their son, Steven, against him and has bad influence on Steven. Alexis believes Blake is self-centered and often attacks her and accuses her of things she didn't do.   \n",
      " Blake believes that Alexis is trying to turn their son, Steven, against him and has bad influence on Steven. Alexis believes Blake is self-centered and often attacks her and accuses her of being selfish \n",
      " Blake believes that Alexis is trying to turn their son, Steven, against him and has bad influence on Steven. Alexis believes Blake is self-centered and often attacks her and accuses her of ||  || things she\n",
      "========================================\n",
      "When Eleanor meets Sawyer, she will be wearing a slightly transparent black robe. Sawyer would like Eleanor to take a picture of herself in the robe, but the transparency is not visible on photos. \n",
      " When Eleanor meets Sawyer, she will be wearing a slightly transparent black robe. Sawyer would like to see Eleanor \n",
      " When Eleanor meets Sawyer, she will be wearing a slightly transparent black robe. Sawyer would || like || like eleanor\n",
      "========================================\n",
      "When Eleanor meets Sawyer, she will be wearing a slightly transparent black robe. Sawyer would like Eleanor to take a picture of herself in the robe, but the transparency is not visible on photos. \n",
      " When Eleanor meets Sawyer, she will be wearing a slightly transparent black robe. Sawyer would like Eleanor to take a picture of herself wearing \n",
      " When Eleanor meets Sawyer, she will be wearing a slightly transparent black robe. Sawyer would like Eleanor to take a picture || of herself || of herself in the\n",
      "========================================\n",
      "When Eleanor meets Sawyer, she will be wearing a slightly transparent black robe. Sawyer would like Eleanor to take a picture of herself in the robe, but the transparency is not visible on photos. \n",
      " When Eleanor meets Sawyer, she will be wearing a slightly transparent black robe. Sawyer would like Eleanor to take a picture of herself wearing the \n",
      " When Eleanor meets Sawyer, she will be wearing a slightly transparent black robe. Sawyer would like Eleanor to take a picture of || herself || herself in the\n",
      "========================================\n",
      "When Eleanor meets Sawyer, she will be wearing a slightly transparent black robe. Sawyer would like Eleanor to take a picture of herself in the robe, but the transparency is not visible on photos. \n",
      " When Eleanor meets Sawyer, she will be wearing a slightly transparent black robe. Sawyer would like Eleanor to take a picture of herself wearing the a \n",
      " When Eleanor meets Sawyer, she will be wearing a slightly transparent black robe. Sawyer would like Eleanor to take a picture of herself ||  || in the\n",
      "========================================\n",
      "When Eleanor meets Sawyer, she will be wearing a slightly transparent black robe. Sawyer would like Eleanor to take a picture of herself in the robe, but the transparency is not visible on photos. \n",
      " When Eleanor meets Sawyer, she will be wearing a slightly transparent black robe. Sawyer would like Eleanor to take a picture of herself in the robe, but the picture doesn's's. \n",
      " When Eleanor meets Sawyer, she will be wearing a slightly transparent black robe. Sawyer would like Eleanor to take a picture of herself in the robe, but the ||  || transparency is\n",
      "========================================\n",
      "When Eleanor meets Sawyer, she will be wearing a slightly transparent black robe. Sawyer would like Eleanor to take a picture of herself in the robe, but the transparency is not visible on photos. \n",
      " When Eleanor meets Sawyer, she will be wearing a slightly transparent black robe. Sawyer would like Eleanor to take a picture of herself in the robe, but the transparency of the \n",
      " When Eleanor meets Sawyer, she will be wearing a slightly transparent black robe. Sawyer would like Eleanor to take a picture of herself in the robe, but the transparency ||  || is not\n",
      "========================================\n",
      "James's girlfriend made him put the trash bin near the kitchen window outside so she can throw things out of the window. \n",
      " James's girlfriend made him put the trash bin near the kitchen window outside so she can throw out garbage easily. \n",
      " James's girlfriend made him put the trash bin near the kitchen window outside so she can throw ||  || things out of the\n",
      "========================================\n",
      "James's girlfriend made him put the trash bin near the kitchen window outside so she can throw things out of the window. \n",
      " James's girlfriend made him put the trash bin near the kitchen window outside so she can throw things out without going outside \n",
      " James's girlfriend made him put the trash bin near the kitchen window outside so she can throw things || out || out of the\n",
      "========================================\n",
      "James's girlfriend made him put the trash bin near the kitchen window outside so she can throw things out of the window. \n",
      " James's girlfriend made him put the trash bin near the kitchen window outside so she can throw things out without going outside. \n",
      " James's girlfriend made him put the trash bin near the kitchen window outside so she can throw things out ||  || of the\n",
      "========================================\n",
      "Francine and Jessie talked yesterday. The talk has been helpful for Jessie. Francine says she's always there for Jessie. Jessie is very thankful. \n",
      " Francine and Jessie talked yesterday. The conversation helped Jessie calm down \n",
      " Francine and Jessie talked yesterday. The ||  || talk\n",
      "========================================\n",
      "Richard took Hannah's parking spot again. Anne saw it. \n",
      " Richard took Hannah's spot again. \n",
      " Richard || took hannah's || took hannah's parking spot again.\n",
      "========================================\n",
      "Richard took Hannah's parking spot again. Anne saw it. \n",
      " Richard took Hannah's spot again. \n",
      " Richard took || hannah's || hannah's parking spot again.\n",
      "========================================\n",
      "Richard took Hannah's parking spot again. Anne saw it. \n",
      " Richard took Hannah's spot again. \n",
      " Richard took Hannah's ||  || parking spot again.\n",
      "========================================\n",
      "Sophie doesn't know what to buy for a gift. She wants Monica's help. She is considering buying a day spa gift card. Monica is bad at buying gifts. \n",
      " Sophie doesn't know what to buy for a gift. She wants to buy a day spa gift. \n",
      " Sophie doesn't know what to buy for a gift. She wants ||  || monica's help.\n",
      "========================================\n",
      "Sophie doesn't know what to buy for a gift. She wants Monica's help. She is considering buying a day spa gift card. Monica is bad at buying gifts. \n",
      " Sophie doesn't know what to buy for a gift. She wants Monica's help. She is considering buying a day spa gift card. \n",
      " Sophie doesn't know what to buy for a gift. She wants Monica's help. She is considering buying a day spa gift || card. || card. monica\n",
      "========================================\n",
      "Sophie doesn't know what to buy for a gift. She wants Monica's help. She is considering buying a day spa gift card. Monica is bad at buying gifts. \n",
      " Sophie doesn't know what to buy for a gift. She wants Monica's help. She is considering buying a day spa gift card. \n",
      " Sophie doesn't know what to buy for a gift. She wants Monica's help. She is considering buying a day spa gift card. ||  || monica is bad at buying gifts.\n",
      "========================================\n",
      "Christmas is coming. Chloe will buy Carter new socks as he wears them unmatched. Victoria bought herself a new piece of gold equipment. \n",
      " Christmas is near. Chloe bought Carter new socks. Victoria gave up for the second dive. \n",
      " Christmas || is || is coming. chloe\n",
      "========================================\n",
      "Christmas is coming. Chloe will buy Carter new socks as he wears them unmatched. Victoria bought herself a new piece of gold equipment. \n",
      " Christmas is near. Chloe bought Carter new socks. Victoria gave up for the same socks. \n",
      " Christmas is ||  || coming. chloe\n",
      "========================================\n",
      "Christmas is coming. Chloe will buy Carter new socks as he wears them unmatched. Victoria bought herself a new piece of gold equipment. \n",
      " Christmas is coming. Chloe will buy Carter new socks as he's nearly Christmas. \n",
      " Christmas is coming. Chloe will buy Carter new socks as he ||  || wears\n",
      "========================================\n",
      "Christmas is coming. Chloe will buy Carter new socks as he wears them unmatched. Victoria bought herself a new piece of gold equipment. \n",
      " Christmas is coming. Chloe will buy Carter new socks as he wears them unmatched. Victoria gave up for the \n",
      " Christmas is coming. Chloe will buy Carter new socks as he wears them unmatched. || victoria || victoria bought herself\n",
      "========================================\n",
      "Christmas is coming. Chloe will buy Carter new socks as he wears them unmatched. Victoria bought herself a new piece of gold equipment. \n",
      " Christmas is coming. Chloe will buy Carter new socks as he wears them unmatched. Victoria gave up for the \n",
      " Christmas is coming. Chloe will buy Carter new socks as he wears them unmatched. Victoria ||  || bought herself a\n",
      "========================================\n",
      "Christmas is coming. Chloe will buy Carter new socks as he wears them unmatched. Victoria bought herself a new piece of gold equipment. \n",
      " Christmas is coming. Chloe will buy Carter new socks as he wears them unmatched. Victoria bought herself a new piece of \n",
      " Christmas is coming. Chloe will buy Carter new socks as he wears them unmatched. Victoria bought herself || a new piece of || a new piece of gold equipment.\n",
      "========================================\n",
      "Ammalee sent Maryann a photo of her nails that lasted over a month. Maryann will try Acrylic instead Linen on Ammalee's nails next time. Ammalee will send Maryann a photo of a design that she would like to have on her nails. \n",
      " Ammalee sent Maryann a photo of her nails. Maryann recommends \n",
      " Ammalee sent Maryann a photo of her nails ||  || that lasted over a\n",
      "========================================\n",
      "Ammalee sent Maryann a photo of her nails that lasted over a month. Maryann will try Acrylic instead Linen on Ammalee's nails next time. Ammalee will send Maryann a photo of a design that she would like to have on her nails. \n",
      " Ammalee sent Maryann a photo of her nails that lasted over a month. Maryann recommends \n",
      " Ammalee sent Maryann a photo of her nails that lasted over a month. || maryann || maryann will\n",
      "========================================\n",
      "Ammalee sent Maryann a photo of her nails that lasted over a month. Maryann will try Acrylic instead Linen on Ammalee's nails next time. Ammalee will send Maryann a photo of a design that she would like to have on her nails. \n",
      " Ammalee sent Maryann a photo of her nails that lasted over a month. Maryann will try to recommend Acrylic \n",
      " Ammalee sent Maryann a photo of her nails that lasted over a month. Maryann will || try || try acrylic\n",
      "========================================\n",
      "Ammalee sent Maryann a photo of her nails that lasted over a month. Maryann will try Acrylic instead Linen on Ammalee's nails next time. Ammalee will send Maryann a photo of a design that she would like to have on her nails. \n",
      " Ammalee sent Maryann a photo of her nails that lasted over a month. Maryann will try to recommend Acrylic \n",
      " Ammalee sent Maryann a photo of her nails that lasted over a month. Maryann will try ||  || acrylic\n",
      "========================================\n",
      "Ammalee sent Maryann a photo of her nails that lasted over a month. Maryann will try Acrylic instead Linen on Ammalee's nails next time. Ammalee will send Maryann a photo of a design that she would like to have on her nails. \n",
      " Ammalee sent Maryann a photo of her nails that lasted over a month. Maryann will try Acrylic instead Linen on Ammalee's nails next time. Ammalee will send Mary \n",
      " Ammalee sent Maryann a photo of her nails that lasted over a month. Maryann will try Acrylic instead Linen on Ammalee's nails next time. Ammalee will || send || send maryann a\n",
      "========================================\n",
      "Ammalee sent Maryann a photo of her nails that lasted over a month. Maryann will try Acrylic instead Linen on Ammalee's nails next time. Ammalee will send Maryann a photo of a design that she would like to have on her nails. \n",
      " Ammalee sent Maryann a photo of her nails that lasted over a month. Maryann will try Acrylic instead Linen on Ammalee's nails next time. Ammalee will send Maryann a photo of a design that she would like to have. \n",
      " Ammalee sent Maryann a photo of her nails that lasted over a month. Maryann will try Acrylic instead Linen on Ammalee's nails next time. Ammalee will send Maryann a photo of a design that she would like to have ||  || on\n",
      "========================================\n",
      "Alan is making his famous chicken wings and would like Joan to be home by 7. Joan won't make it as she is studying with Laura. \n",
      " Alan is making his famous chicken wings and would like Joan to come home by 7. Joan is studying. \n",
      " Alan is making his famous chicken wings and would like Joan to ||  || be home by 7. joan\n",
      "========================================\n",
      "Alan is making his famous chicken wings and would like Joan to be home by 7. Joan won't make it as she is studying with Laura. \n",
      " Alan is making his famous chicken wings and would like Joan to be home by 7. Joan won't make it because her grades are more important than \n",
      " Alan is making his famous chicken wings and would like Joan to be home by 7. Joan won't make || it || it as she is\n",
      "========================================\n",
      "Alan is making his famous chicken wings and would like Joan to be home by 7. Joan won't make it as she is studying with Laura. \n",
      " Alan is making his famous chicken wings and would like Joan to be home by 7. Joan won't make it as she's studying. \n",
      " Alan is making his famous chicken wings and would like Joan to be home by 7. Joan won't make it as ||  || she is studying with\n",
      "========================================\n",
      "Johny wants to go out clubbing with Luke in order to meet a female partner. Luke has plans but he advises Johny to install Tinder. Luke critisizes Johny for speaking about women without much respect. \n",
      " Johny wants to date a girl on Tuesday. Luke doesn's to date. \n",
      " Johny wants || to || to go\n",
      "========================================\n",
      "Johny wants to go out clubbing with Luke in order to meet a female partner. Luke has plans but he advises Johny to install Tinder. Luke critisizes Johny for speaking about women without much respect. \n",
      " Johny wants to date a girl on Tuesday. Luke doesn't's. \n",
      " Johny wants to ||  || go\n",
      "========================================\n",
      "Johny wants to go out clubbing with Luke in order to meet a female partner. Luke has plans but he advises Johny to install Tinder. Luke critisizes Johny for speaking about women without much respect. \n",
      " Johny wants to go out with Luke tonight. Luke doesn't want to the main chick. \n",
      " Johny wants to go out ||  || clubbing with\n",
      "========================================\n",
      "Johny wants to go out clubbing with Luke in order to meet a female partner. Luke has plans but he advises Johny to install Tinder. Luke critisizes Johny for speaking about women without much respect. \n",
      " Johny wants to go out clubbing with Luke in order to meet a female partner. Luke doesn't \n",
      " Johny wants to go out clubbing with Luke in order to meet a female partner. || luke || luke has\n",
      "========================================\n",
      "Johny wants to go out clubbing with Luke in order to meet a female partner. Luke has plans but he advises Johny to install Tinder. Luke critisizes Johny for speaking about women without much respect. \n",
      " Johny wants to go out clubbing with Luke in order to meet a female partner. Luke doesn't want to the girl on tonight. \n",
      " Johny wants to go out clubbing with Luke in order to meet a female partner. Luke ||  || has\n",
      "========================================\n",
      "Johny wants to go out clubbing with Luke in order to meet a female partner. Luke has plans but he advises Johny to install Tinder. Luke critisizes Johny for speaking about women without much respect. \n",
      " Johny wants to go out clubbing with Luke in order to meet a female partner. Luke has other plans for tonight. \n",
      " Johny wants to go out clubbing with Luke in order to meet a female partner. Luke has ||  || plans\n",
      "========================================\n",
      "Johny wants to go out clubbing with Luke in order to meet a female partner. Luke has plans but he advises Johny to install Tinder. Luke critisizes Johny for speaking about women without much respect. \n",
      " Johny wants to go out clubbing with Luke in order to meet a female partner. Luke has plans but he advises Johny not to go to \n",
      " Johny wants to go out clubbing with Luke in order to meet a female partner. Luke has plans but he advises Johny ||  || to\n",
      "========================================\n",
      "Rosie is on her way to meet Frank and will have to find him at Sports Direct. \n",
      " Rosie is on her way to meet Frank and Peter at Sports Direct store at the \n",
      " Rosie is on her way to meet Frank and ||  || will\n",
      "========================================\n",
      "Rosie is on her way to meet Frank and will have to find him at Sports Direct. \n",
      " Rosie is on her way to meet Frank and will have to find him at Sports Direct store at the \n",
      " Rosie is on her way to meet Frank and will have to find him || at sports || at sports direct.\n",
      "========================================\n",
      "Rosie is on her way to meet Frank and will have to find him at Sports Direct. \n",
      " Rosie is on her way to meet Frank and will have to find him at Sports Direct store. \n",
      " Rosie is on her way to meet Frank and will have to find him at || sports || sports direct.\n",
      "========================================\n",
      "Kyle reminds Patrick about their math homework for tomorrow. \n",
      " Kyle reminded Patrick that maths homework is for tomorrow. \n",
      " Kyle ||  || reminds patrick\n",
      "========================================\n",
      "Kyle reminds Patrick about their math homework for tomorrow. \n",
      " Kyle reminds Patrick about their maths homework for tomorrow. \n",
      " Kyle reminds Patrick about their ||  || math homework for tomorrow.\n",
      "========================================\n",
      "Ellen and Paul are doing the redecoration and it takes a lot of time. They want to finish it by Christmas but workers are permanently late. Kate is looking forward for the housewarming party. \n",
      " Ellen and Paul are doing a redecoration of their kitchen. \n",
      " Ellen and Paul are doing ||  || the redecoration\n",
      "========================================\n",
      "Ellen and Paul are doing the redecoration and it takes a lot of time. They want to finish it by Christmas but workers are permanently late. Kate is looking forward for the housewarming party. \n",
      " Ellen and Paul are doing the redecoration and it takes a lot of time. They want to finish it by Christmas. \n",
      " Ellen and Paul are doing the redecoration and it takes a lot of time. They want to || finish it by || finish it by christmas but\n",
      "========================================\n",
      "Ellen and Paul are doing the redecoration and it takes a lot of time. They want to finish it by Christmas but workers are permanently late. Kate is looking forward for the housewarming party. \n",
      " Ellen and Paul are doing the redecoration and it takes a lot of time. They want to finish it by Christmas but workers are always late. \n",
      " Ellen and Paul are doing the redecoration and it takes a lot of time. They want to finish it by Christmas but workers || are || are permanently late.\n",
      "========================================\n",
      "Ellen and Paul are doing the redecoration and it takes a lot of time. They want to finish it by Christmas but workers are permanently late. Kate is looking forward for the housewarming party. \n",
      " Ellen and Paul are doing the redecoration and it takes a lot of time. They want to finish it by Christmas but workers are always late. \n",
      " Ellen and Paul are doing the redecoration and it takes a lot of time. They want to finish it by Christmas but workers are ||  || permanently late.\n",
      "========================================\n",
      "Paola and her friends are going to the theatre El Rincón de Sánchez on Friday. They are going to see a play about Serbian family just after the war in Yugoslavia. Nicola and Austin will go to the theatre together. They will eat empanadas on their way. \n",
      " Paola and her friends are going to the theatre to see a play is going to see a play about a. \n",
      " Paola and her || friends are going to the theatre || friends are going to the theatre el rincón\n",
      "========================================\n",
      "Paola and her friends are going to the theatre El Rincón de Sánchez on Friday. They are going to see a play about Serbian family just after the war in Yugoslavia. Nicola and Austin will go to the theatre together. They will eat empanadas on their way. \n",
      " Paola and her friends are going to the theatre to see a play is going to see a play about a. \n",
      " Paola and her friends || are going to the theatre || are going to the theatre el rincón\n",
      "========================================\n",
      "Paola and her friends are going to the theatre El Rincón de Sánchez on Friday. They are going to see a play about Serbian family just after the war in Yugoslavia. Nicola and Austin will go to the theatre together. They will eat empanadas on their way. \n",
      " Paola and her friends are going to the theatre to see a play about the the next Friday at 8.15. \n",
      " Paola and her friends are || going to the theatre || going to the theatre el rincón\n",
      "========================================\n",
      "Paola and her friends are going to the theatre El Rincón de Sánchez on Friday. They are going to see a play about Serbian family just after the war in Yugoslavia. Nicola and Austin will go to the theatre together. They will eat empanadas on their way. \n",
      " Paola and her friends are going to the theatre to see a play called El Rinc. \n",
      " Paola and her friends are going || to the theatre || to the theatre el rincón de\n",
      "========================================\n",
      "Paola and her friends are going to the theatre El Rincón de Sánchez on Friday. They are going to see a play about Serbian family just after the war in Yugoslavia. Nicola and Austin will go to the theatre together. They will eat empanadas on their way. \n",
      " Paola and her friends are going to the theatre to see a play about the theatre to see a play El Rinc. \n",
      " Paola and her friends are going to || the theatre || the theatre el rincón de\n",
      "========================================\n",
      "Paola and her friends are going to the theatre El Rincón de Sánchez on Friday. They are going to see a play about Serbian family just after the war in Yugoslavia. Nicola and Austin will go to the theatre together. They will eat empanadas on their way. \n",
      " Paola and her friends are going to the theatre to see a play about a to see a Serbian to see a a. \n",
      " Paola and her friends are going to the || theatre || theatre el rincón de\n",
      "========================================\n",
      "Paola and her friends are going to the theatre El Rincón de Sánchez on Friday. They are going to see a play about Serbian family just after the war in Yugoslavia. Nicola and Austin will go to the theatre together. They will eat empanadas on their way. \n",
      " Paola and her friends are going to the theatre El Rincón de Sánchez on Friday. They are going to meet at the theatre \n",
      " Paola and her friends are going to the theatre El Rincón de Sánchez on Friday. They are going || to || to see a\n",
      "========================================\n",
      "Paola and her friends are going to the theatre El Rincón de Sánchez on Friday. They are going to see a play about Serbian family just after the war in Yugoslavia. Nicola and Austin will go to the theatre together. They will eat empanadas on their way. \n",
      " Paola and her friends are going to the theatre El Rincón de Sánchez on Friday. They are going to see a play \n",
      " Paola and her friends are going to the theatre El Rincón de Sánchez on Friday. They are going to || see a play || see a play about\n",
      "========================================\n",
      "Vincent and Charlotte are studying in the cloister today. Harriet and Ben will go to the library.  \n",
      " Vincent and Charlotte are studying in the cloister today. Harriet and Ben are going to the. \n",
      " Vincent and Charlotte are studying in the cloister today. Harriet and || ben || ben will go to the\n",
      "========================================\n",
      "Jack has to find Olivia in carriage 3, because she has their tickets. \n",
      " Jack has to go to the airport. Olivia has the tickets. \n",
      " Jack has to ||  || find\n",
      "========================================\n",
      "Jack has to find Olivia in carriage 3, because she has their tickets. \n",
      " Jack has to find Olivia. Olivia has the tickets. \n",
      " Jack has to find ||  || olivia in carriage\n",
      "========================================\n",
      "Jack has to find Olivia in carriage 3, because she has their tickets. \n",
      " Jack has to find Olivia because he is in carriage 3 and it's tickets. \n",
      " Jack has to find Olivia ||  || in carriage\n",
      "========================================\n",
      "Jack has to find Olivia in carriage 3, because she has their tickets. \n",
      " Jack has to find Olivia in carriage 3, he can't find his tickets to go to the tickets. \n",
      " Jack has to find Olivia in carriage 3, ||  || because she has\n",
      "========================================\n",
      "Jack has to find Olivia in carriage 3, because she has their tickets. \n",
      " Jack has to find Olivia in carriage 3, because it's crowded. Olivia has the tickets. \n",
      " Jack has to find Olivia in carriage 3, because ||  || she has\n",
      "========================================\n",
      "Will gived Rob the number of the car mechanic who fixed Will's father's car. \n",
      " Will gived Rob the number of the car mechanic who fixed Will's father's car \n",
      " Will gived Rob the number of the car mechanic who fixed || will's father's || will's father's car.\n",
      "========================================\n",
      "Andrew, Nicky and Rick had a hard week at work. They just want to rest and do nothing. Andrew has a new project next month, and now he just wants to sleep and see kids. \n",
      " Andrew, Nicky and Rick had a hard week at work. They just want to rest and do nothing. Andrew has a new project next month, and now they need  \n",
      " Andrew, Nicky and Rick had a hard week at work. They just want to rest and do nothing. Andrew has a new project next month, and now ||  || he\n",
      "========================================\n",
      "Olivia doesn't know how to caption the photo she wants to put on Instagram. Taylor gives Olivia some hints. \n",
      " Olivia doesn't know how to caption the photo she sent yesterday by the lake. \n",
      " Olivia doesn't know how to caption the photo || she || she wants to\n",
      "========================================\n",
      "Olivia doesn't know how to caption the photo she wants to put on Instagram. Taylor gives Olivia some hints. \n",
      " Olivia doesn't know how to caption the photo she sent yesterday by the lake. Taylor's. \n",
      " Olivia doesn't know how to caption the photo she ||  || wants to\n",
      "========================================\n",
      "Ali is coming to Kane to get the hard drive. \n",
      " Ali is coming to Kane's place to take the hard drive \n",
      " Ali is coming || to || to kane to\n",
      "========================================\n",
      "Ali is coming to Kane to get the hard drive. \n",
      " Ali is coming to Kane's place to take the hard drive. \n",
      " Ali is coming to ||  || kane to\n",
      "========================================\n",
      "During early lunchtime, Shelly will come to Porter to take a look at wool fall coats which have arrived today.  \n",
      " During early lunchtime, Shelly will come to Porter to look for \n",
      " During early lunchtime, || shelly will come to porter to || shelly will come to porter to take\n",
      "========================================\n",
      "During early lunchtime, Shelly will come to Porter to take a look at wool fall coats which have arrived today.  \n",
      " During early lunchtime, Shelly will come to Porter to look for  \n",
      " During early lunchtime, Shelly || will come to porter to || will come to porter to take a\n",
      "========================================\n",
      "During early lunchtime, Shelly will come to Porter to take a look at wool fall coats which have arrived today.  \n",
      " During early lunchtime, Shelly will come to Porter to look for a \n",
      " During early lunchtime, Shelly will || come to porter to || come to porter to take a look\n",
      "========================================\n",
      "During early lunchtime, Shelly will come to Porter to take a look at wool fall coats which have arrived today.  \n",
      " During early lunchtime, Shelly will come to Porter to look for a \n",
      " During early lunchtime, Shelly will come || to porter to || to porter to take a look\n",
      "========================================\n",
      "During early lunchtime, Shelly will come to Porter to take a look at wool fall coats which have arrived today.  \n",
      " During early lunchtime, Shelly will come to Porter to look for a fall coats. \n",
      " During early lunchtime, Shelly will come to || porter to || porter to take a look at\n",
      "========================================\n",
      "During early lunchtime, Shelly will come to Porter to take a look at wool fall coats which have arrived today.  \n",
      " During early lunchtime, Shelly will come to Porter to look for a fall coats. \n",
      " During early lunchtime, Shelly will come to Porter || to || to take a look at\n",
      "========================================\n",
      "During early lunchtime, Shelly will come to Porter to take a look at wool fall coats which have arrived today.  \n",
      " During early lunchtime, Shelly will come to Porter to take a look at some woolen \n",
      " During early lunchtime, Shelly will come to Porter to take || a look at || a look at wool fall\n",
      "========================================\n",
      "During early lunchtime, Shelly will come to Porter to take a look at wool fall coats which have arrived today.  \n",
      " During early lunchtime, Shelly will come to Porter to take a look at some wool fall \n",
      " During early lunchtime, Shelly will come to Porter to take a || look at || look at wool fall\n",
      "========================================\n",
      "During early lunchtime, Shelly will come to Porter to take a look at wool fall coats which have arrived today.  \n",
      " During early lunchtime, Shelly will come to Porter to take a look at some wool fall coat \n",
      " During early lunchtime, Shelly will come to Porter to take a look || at || at wool fall coats\n",
      "========================================\n",
      "During early lunchtime, Shelly will come to Porter to take a look at wool fall coats which have arrived today.  \n",
      " During early lunchtime, Shelly will come to Porter to take a look at some wool fall coats \n",
      " During early lunchtime, Shelly will come to Porter to take a look at ||  || wool fall\n",
      "========================================\n",
      "Connor is looking for a playlist from the Berlin concert. Kyle directs him to the band's official Twitter account. \n",
      " Connor is interested in the set list of the band at the gig in the songs they played at the gig in Berlin. \n",
      " Connor is ||  || looking for\n",
      "========================================\n",
      "Jeniffer and Brooke're in New York now. They've been also to Connecticut, Massachusetts and Rhode Island. The thing Jeniffer likes the most in America is diversity while in Brooke's opinion it's urban splendor. \n",
      " Jeniffer and Brooke're in New York now. They've been also to Connecticut, Massachusetts and Rhode Island. The thing they like the city is the most is the most is the most is the most is the most is the most is the most. \n",
      " Jeniffer and Brooke're in New York now. They've been also to Connecticut, Massachusetts and Rhode Island. The thing ||  || jeniffer\n",
      "========================================\n",
      "Jeniffer and Brooke're in New York now. They've been also to Connecticut, Massachusetts and Rhode Island. The thing Jeniffer likes the most in America is diversity while in Brooke's opinion it's urban splendor. \n",
      " Jeniffer and Brooke're in New York now. They've been also to Connecticut, Massachusetts and Rhode Island. The thing Jeniffer likes the city is the most is \n",
      " Jeniffer and Brooke're in New York now. They've been also to Connecticut, Massachusetts and Rhode Island. The thing Jeniffer || likes the || likes the most\n",
      "========================================\n",
      "Max is sorry about his behaviour so wants to meet up with Lucas and Mason. Lucas will let him know.  \n",
      " Max is sorry about his behaviour so wants to meet up with Lucas and Mason to talk it \n",
      " Max is sorry about his behaviour so wants to meet up || with lucas and || with lucas and mason.\n",
      "========================================\n",
      "Max is sorry about his behaviour so wants to meet up with Lucas and Mason. Lucas will let him know.  \n",
      " Max is sorry about his behaviour so wants to meet up with Lucas and Mason to talk it through. \n",
      " Max is sorry about his behaviour so wants to meet up with || lucas and || lucas and mason.\n",
      "========================================\n",
      "Max is sorry about his behaviour so wants to meet up with Lucas and Mason. Lucas will let him know.  \n",
      " Max is sorry about his behaviour so wants to meet up with Lucas and Mason to talk it through \n",
      " Max is sorry about his behaviour so wants to meet up with Lucas || and || and mason.\n",
      "========================================\n",
      "Max is sorry about his behaviour so wants to meet up with Lucas and Mason. Lucas will let him know.  \n",
      " Max is sorry about his behaviour so wants to meet up with Lucas and Mason to talk it through. \n",
      " Max is sorry about his behaviour so wants to meet up with Lucas and ||  || mason.\n",
      "========================================\n",
      "Max is sorry about his behaviour so wants to meet up with Lucas and Mason. Lucas will let him know.  \n",
      " Max is sorry about his behaviour so wants to meet up with Lucas and Mason. Lucas will think about it and \n",
      " Max is sorry about his behaviour so wants to meet up with Lucas and Mason. Lucas || will || will let\n",
      "========================================\n",
      "Max is sorry about his behaviour so wants to meet up with Lucas and Mason. Lucas will let him know.  \n",
      " Max is sorry about his behaviour so wants to meet up with Lucas and Mason. Lucas will think about it and let him. \n",
      " Max is sorry about his behaviour so wants to meet up with Lucas and Mason. Lucas will ||  || let\n",
      "========================================\n",
      "Max is sorry about his behaviour so wants to meet up with Lucas and Mason. Lucas will let him know.  \n",
      " Max is sorry about his behaviour so wants to meet up with Lucas and Mason. Lucas will let Max know. \n",
      " Max is sorry about his behaviour so wants to meet up with Lucas and Mason. Lucas will let ||  || him know.\n",
      "========================================\n",
      "O'Neill is worried about not having heard from Ted. Ted is fine and is going to send a photo later. \n",
      " O'Neill is not hearing back from Ted. Ted has been busy today. \n",
      " O'Neill is ||  || worried\n",
      "========================================\n",
      "O'Neill is worried about not having heard from Ted. Ted is fine and is going to send a photo later. \n",
      " O'Neill is worried about not having a photo with Ted. Ted will sends. \n",
      " O'Neill is worried about not having ||  || heard from ted. ted\n",
      "========================================\n",
      "O'Neill is worried about not having heard from Ted. Ted is fine and is going to send a photo later. \n",
      " O'Neill is worried about not having heard back from Ted. Ted has been busy today. \n",
      " O'Neill is worried about not having heard ||  || from ted. ted\n",
      "========================================\n",
      "O'Neill is worried about not having heard from Ted. Ted is fine and is going to send a photo later. \n",
      " O'Neill is worried about not having heard from Ted. Ted has been busy today. Ted is fine. \n",
      " O'Neill is worried about not having heard from Ted. Ted ||  || is\n",
      "========================================\n",
      "Sandra is setting into the new house; her family is happy with it. Then Sandra and Gabby discuss the nature of their men and laugh about their habit of spending time in the garage or a shed. \n",
      " Sandra is setting up her new house. The kids and the rest of course. \n",
      " Sandra is setting ||  || into the new\n",
      "========================================\n",
      "Sandra is setting into the new house; her family is happy with it. Then Sandra and Gabby discuss the nature of their men and laugh about their habit of spending time in the garage or a shed. \n",
      " Sandra is setting into the new house; her family is happy with it. Then Sandra and Gabby's husband Jim's. \n",
      " Sandra is setting into the new house; her family is happy with it. Then Sandra and ||  || gabby\n",
      "========================================\n",
      "Sandra is setting into the new house; her family is happy with it. Then Sandra and Gabby discuss the nature of their men and laugh about their habit of spending time in the garage or a shed. \n",
      " Sandra is setting into the new house; her family is happy with it. Then Sandra and Gabby meet Jim. Jim's. \n",
      " Sandra is setting into the new house; her family is happy with it. Then Sandra and Gabby ||  || discuss\n",
      "========================================\n",
      "Sandra is setting into the new house; her family is happy with it. Then Sandra and Gabby discuss the nature of their men and laugh about their habit of spending time in the garage or a shed. \n",
      " Sandra is setting into the new house; her family is happy with it. Then Sandra and Gabby discuss the nature of their men and laugh about Jim. \n",
      " Sandra is setting into the new house; her family is happy with it. Then Sandra and Gabby discuss the nature of their men and laugh about ||  || their\n",
      "========================================\n",
      "Sandra is setting into the new house; her family is happy with it. Then Sandra and Gabby discuss the nature of their men and laugh about their habit of spending time in the garage or a shed. \n",
      " Sandra is setting into the new house; her family is happy with it. Then Sandra and Gabby discuss the nature of their men and laugh about their habit of spending a lot. \n",
      " Sandra is setting into the new house; her family is happy with it. Then Sandra and Gabby discuss the nature of their men and laugh about their habit of spending ||  || time in the\n",
      "========================================\n",
      "-264 0 -20 -1.4828793558960458\n"
     ]
    }
   ],
   "source": [
    "normal_verbatims = 0\n",
    "stopword_verbatims = 0\n",
    "gender_verbatims = 0\n",
    "log_proba_diff = 0\n",
    "for i, (r1, r2) in enumerate(results):\n",
    "    wl = r1[\"word_index_list\"]\n",
    "    log_proba_diff += ((r2[\"log_proba_of_label\"] - r1[\"log_proba_of_label\"])/len(wl))\n",
    "    for ix in wl:\n",
    "        if r2[ix][\"generation_length_verbatim\"] < r1[ix][\"generation_length_verbatim\"]:\n",
    "            print(r2[\"label\"], \n",
    "                  \"\\n\", r2[ix][\"predictions\"],\n",
    "                  \"\\n\", r2[ix][\"prefix_text\"],\"||\", \n",
    "                  r2[ix][\"generation_suffix_verbatim_text\"], \"||\", r1[ix][\"generation_suffix_verbatim_text\"])\n",
    "            print(\"=\"*40)\n",
    "        # r2[ix][\"generation_length_verbatim\"], r1[ix][\"generation_length_verbatim\"]\n",
    "        normal_verbatims += (r2[ix][\"generation_length_verbatim\"] - r1[ix][\"generation_length_verbatim\"])\n",
    "        \n",
    "        # if \"stopword_repeat\" in r1[ix]:\n",
    "        #     stopword_verbatims+= (r2[ix][\"stopword_repeat\"][\"generation_length_verbatim\"] - r1[ix][\"stopword_repeat\"][\"generation_length_verbatim\"])\n",
    "        if \"gender_swap\" in r1[ix]:\n",
    "            gender_verbatims += (r2[ix][\"gender_swap\"][\"generation_length_verbatim\"] - r1[ix][\"gender_swap\"][\"generation_length_verbatim\"])\n",
    "print(normal_verbatims, stopword_verbatims, gender_verbatims, log_proba_diff/len(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da70e762-957f-424c-ba11-48d93056414b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# idx = random.randint(0, len(dataset[\"train\"]))\n",
    "label_text = dataset[\"train\"][idx][\"summary\"]\n",
    "input_text = f'summarize: {dataset[\"train\"][idx][\"dialogue\"]}'\n",
    "\n",
    "\n",
    "# Label is a prefix of generation\n",
    "# Can I tell it the number of words\n",
    "run_for_one_model(models[0], \"0\", idx, input_text, label_text, tokenizer, padding, device, **gen_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4347e1-807b-43bf-8eea-d3271effe83a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_for_one_model(models[1], \"1\", input_text, label_text, tokenizer, padding, device, **gen_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9f022972-6a0f-4a52-a7b8-a2d98cff78b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T16:57:03.650408Z",
     "iopub.status.busy": "2023-05-09T16:57:03.650047Z",
     "iopub.status.idle": "2023-05-09T16:57:03.667328Z",
     "shell.execute_reply": "2023-05-09T16:57:03.666587Z",
     "shell.execute_reply.started": "2023-05-09T16:57:03.650380Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen_kwargs = {\n",
    "                \"max_length\": max_target_length,\n",
    "                \"num_beams\": 1,\n",
    "    }\n",
    "\n",
    "\n",
    "def investigate(idx, input_text, label_text, models, generator, tokenizer, gen_kwargs, padding, fold, percent_frac, **more_gen_kwargs):\n",
    "    gen_kwargs = deepcopy(gen_kwargs)\n",
    "    gen_kwargs.update(more_gen_kwargs)\n",
    "    labels = tokenizer(text_target=[label_text], max_length=max_target_length, padding=padding, truncation=True)\n",
    "    label_ids = labels[\"input_ids\"][0]\n",
    "    labels = [tokenizer.decode(i) for i in label_ids]\n",
    "    model_texts = dict()\n",
    "    prefix_length = int(len(label_text.split()) * percent_frac)\n",
    "    prefix_text = \" \".join(label_text.split()[:prefix_length])\n",
    "    model_texts[\"input\"] = (input_text,prefix_text, 0, 0)\n",
    "    model_texts[\"label\"] = (label_text,prefix_text, 0, 0)\n",
    "    \n",
    "    \n",
    "    # pipeline_generation = generator(input_text, **gen_kwargs)\n",
    "    # pipeline_token_ids = tokenizer(text_target=[pipeline_generation[0]['generated_text']], max_length=gen_kwargs[\"max_length\"], padding=padding, truncation=True)\n",
    "    # pipeline_token_ids = pipeline_token_ids[\"input_ids\"][0]\n",
    "    # pipeline_decoded = [tokenizer.decode(i) for i in pipeline_token_ids]\n",
    "    # model_texts[\"pipeline\"] = (pipeline_generation[0]['generated_text'],prefix_text, 0, 0)\n",
    "\n",
    "    \n",
    "    batch = tokenizer(input_text, max_length=max_length, padding=padding, truncation=True, return_tensors=\"pt\")\n",
    "    model_predictions = dict()\n",
    "\n",
    "    for FOLD, used_model in models.items():\n",
    "        with torch.no_grad():\n",
    "            generated_ids = used_model.generate(\n",
    "                            input_ids=batch[\"input_ids\"],\n",
    "                            attention_mask=batch[\"attention_mask\"],\n",
    "                            use_cache=True,\n",
    "                            **gen_kwargs,\n",
    "                        )\n",
    "            \n",
    "            one_steps_probas = get_one_step_proba(used_model, \n",
    "                                                  tokenizer(text_target=[label_text], max_length=max_target_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")[\"input_ids\"], \n",
    "                                                  batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "            probas = one_steps_probas[\"probas\"]\n",
    "            if len(probas) < max_target_length:\n",
    "                probas = probas + ([0] * (max_target_length - len(probas)))\n",
    "            \n",
    "            log_proba_of_label = one_steps_probas[\"log_proba\"]\n",
    "        generated_ids = generated_ids.squeeze().tolist()\n",
    "        if generated_ids[0] == tokenizer.pad_token_id:\n",
    "            generated_ids = generated_ids[1:]\n",
    "        if len(generated_ids) < max_target_length:\n",
    "            generated_ids = generated_ids + [tokenizer.pad_token_id]*(max_target_length - len(generated_ids))\n",
    "        generated_ids_decoded = [tokenizer.decode(i) for i in generated_ids]\n",
    "        model_predictions[f\"generated_token_ids_{FOLD}\"] = generated_ids\n",
    "        model_predictions[f\"generated_tokens_{FOLD}\"] = generated_ids_decoded\n",
    "        model_predictions[f\"generated_probas_{FOLD}\"] = probas\n",
    "        predictions = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "        \n",
    "        greedy_prefix_decode = greedy_prefix_decoding(used_model, tokenizer, prefix_text, batch[\"input_ids\"], batch[\"attention_mask\"], max_target_length).squeeze().tolist()\n",
    "        greedy_prefix_predictions = tokenizer.decode(greedy_prefix_decode, skip_special_tokens=True)\n",
    "        \n",
    "        one_steps_probas = get_one_step_proba(used_model, \n",
    "                                                  tokenizer(text_target=[predictions], max_length=max_target_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")[\"input_ids\"], \n",
    "                                                  batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "        log_proba_of_generated = one_steps_probas[\"log_proba\"]\n",
    "        model_texts[FOLD] = (predictions, greedy_prefix_predictions, log_proba_of_label, log_proba_of_generated)\n",
    "        \n",
    "    rouge_scores = dict()\n",
    "    for FOLD, (predictions, greedy_prefix_predictions, log_proba_of_label, log_proba_of_generated) in model_texts.items():\n",
    "        rouge_score = metric.compute(predictions=[predictions], references=[label_text], use_stemmer=True)\n",
    "        rouge_score[\"text\"] = predictions\n",
    "        rouge_score[\"prefix_text\"] = greedy_prefix_predictions\n",
    "        rouge_score[\"prefix_text_match_label\"] = greedy_prefix_predictions.strip().lower() == label_text.strip().lower()\n",
    "        rouge_score[\"rouge_of_prefix_text\"] = metric.compute(predictions=[greedy_prefix_predictions], references=[label_text], use_stemmer=True)\n",
    "        rouge_score[\"log_proba_of_label\"] = log_proba_of_label\n",
    "        rouge_score[\"log_proba_of_generated\"] = log_proba_of_generated\n",
    "        rouge_scores[FOLD] = rouge_score\n",
    "    rouge_df = pd.DataFrame(rouge_scores.values(), index=rouge_scores.keys())\n",
    "    rouge_df\n",
    "    if fold == \"train\":\n",
    "        probas_dict = combined_ds[idx]\n",
    "        pdict = dict()\n",
    "        # [x.replace(\"_windowed\", \"\") for x in models.keys() if \"_windowed\" in str(x)] +\n",
    "        for k in [\"proba0\", \"proba1\"] + [x for x in models.keys()]:\n",
    "            if k in probas_dict:\n",
    "                pdict[k] = probas_dict[k]\n",
    "        probas_dict = pdict\n",
    "    else:\n",
    "        probas_dict = dict()\n",
    "    our_dict = {\n",
    "                \"labels\": labels, \n",
    "                        # \"label_ids\": label_ids, \n",
    "                        # \"pipeline_token_ids\": pipeline_token_ids, \"pipeline_tokens\": pipeline_decoded, \n",
    "                **model_predictions}\n",
    "\n",
    "    probas_dict.update(our_dict)\n",
    "    # print({k: len(v) for k, v in probas_dict.items()})\n",
    "    df = pd.DataFrame(probas_dict)\n",
    "    df = df.loc[((df==tokenizer.pad_token).sum(axis=1) < 6)|(df[\"labels\"]!=tokenizer.pad_token)]\n",
    "    return rouge_df, df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a22fef94-c447-4daa-a666-7780d154027b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-07T13:53:15.590647Z",
     "iopub.status.busy": "2023-05-07T13:53:15.590389Z",
     "iopub.status.idle": "2023-05-07T13:54:29.292856Z",
     "shell.execute_reply": "2023-05-07T13:54:29.291780Z",
     "shell.execute_reply.started": "2023-05-07T13:53:15.590622Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 73.6908 s\n",
       "File: /tmp/ipykernel_84155/3118769970.py\n",
       "Function: investigate at line 7\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     7                                           def investigate(idx, input_text, label_text, models, generator, tokenizer, gen_kwargs, padding, fold, percent_frac, **more_gen_kwargs):\n",
       "     8         1      28108.0  28108.0      0.0      gen_kwargs = deepcopy(gen_kwargs)\n",
       "     9         1       1852.0   1852.0      0.0      gen_kwargs.update(more_gen_kwargs)\n",
       "    10         1     574587.0 574587.0      0.0      labels = tokenizer(text_target=[label_text], max_length=max_target_length, padding=padding, truncation=True)\n",
       "    11         1       4771.0   4771.0      0.0      label_ids = labels[\"input_ids\"][0]\n",
       "    12         1    1585692.0 1585692.0      0.0      labels = [tokenizer.decode(i) for i in label_ids]\n",
       "    13         1        611.0    611.0      0.0      model_texts = dict()\n",
       "    14         1       3940.0   3940.0      0.0      prefix_length = int(len(label_text.split()) * percent_frac)\n",
       "    15         1       3066.0   3066.0      0.0      prefix_text = \" \".join(label_text.split()[:prefix_length])\n",
       "    16         1       1215.0   1215.0      0.0      model_texts[\"input\"] = (input_text,prefix_text, 0, 0)\n",
       "    17         1        921.0    921.0      0.0      model_texts[\"label\"] = (label_text,prefix_text, 0, 0)\n",
       "    18                                               \n",
       "    19                                               \n",
       "    20                                               # pipeline_generation = generator(input_text, **gen_kwargs)\n",
       "    21                                               # pipeline_token_ids = tokenizer(text_target=[pipeline_generation[0]['generated_text']], max_length=gen_kwargs[\"max_length\"], padding=padding, truncation=True)\n",
       "    22                                               # pipeline_token_ids = pipeline_token_ids[\"input_ids\"][0]\n",
       "    23                                               # pipeline_decoded = [tokenizer.decode(i) for i in pipeline_token_ids]\n",
       "    24                                               # model_texts[\"pipeline\"] = (pipeline_generation[0]['generated_text'],prefix_text, 0, 0)\n",
       "    25                                           \n",
       "    26                                               \n",
       "    27         1     929642.0 929642.0      0.0      batch = tokenizer(input_text, max_length=max_length, padding=padding, truncation=True, return_tensors=\"pt\")\n",
       "    28         1       1251.0   1251.0      0.0      model_predictions = dict()\n",
       "    29                                           \n",
       "    30        16      42994.0   2687.1      0.0      for FOLD, used_model in models.items():\n",
       "    31        16     299029.0  18689.3      0.0          with torch.no_grad():\n",
       "    32        16 26162960151.0 1635185009.4     35.5              generated_ids = used_model.generate(\n",
       "    33        16     149396.0   9337.2      0.0                              input_ids=batch[\"input_ids\"],\n",
       "    34        16      24487.0   1530.4      0.0                              attention_mask=batch[\"attention_mask\"],\n",
       "    35        16      11488.0    718.0      0.0                              use_cache=True,\n",
       "    36        16      11690.0    730.6      0.0                              **gen_kwargs,\n",
       "    37                                                                   )\n",
       "    38                                                       \n",
       "    39        16 10013450622.0 625840663.9     13.6              one_steps_probas = get_one_step_proba(used_model, \n",
       "    40        16    8865300.0 554081.2      0.0                                                    tokenizer(text_target=[label_text], max_length=max_target_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")[\"input_ids\"], \n",
       "    41        16      56509.0   3531.8      0.0                                                    batch[\"input_ids\"], batch[\"attention_mask\"])\n",
       "    42        16      36201.0   2262.6      0.0              probas = one_steps_probas[\"probas\"]\n",
       "    43        16      45997.0   2874.8      0.0              if len(probas) < max_target_length:\n",
       "    44        16      94693.0   5918.3      0.0                  probas = probas + ([0] * (max_target_length - len(probas)))\n",
       "    45                                                       \n",
       "    46        16      14903.0    931.4      0.0              log_proba_of_label = one_steps_probas[\"log_proba\"]\n",
       "    47        16     333150.0  20821.9      0.0          generated_ids = generated_ids.squeeze().tolist()\n",
       "    48        16     402768.0  25173.0      0.0          if generated_ids[0] == tokenizer.pad_token_id:\n",
       "    49        16      31490.0   1968.1      0.0              generated_ids = generated_ids[1:]\n",
       "    50        16      17408.0   1088.0      0.0          if len(generated_ids) < max_target_length:\n",
       "    51        16     115753.0   7234.6      0.0              generated_ids = generated_ids + [tokenizer.pad_token_id]*(max_target_length - len(generated_ids))\n",
       "    52        16   26213091.0 1638318.2      0.0          generated_ids_decoded = [tokenizer.decode(i) for i in generated_ids]\n",
       "    53        16      68125.0   4257.8      0.0          model_predictions[f\"generated_token_ids_{FOLD}\"] = generated_ids\n",
       "    54        16      23663.0   1478.9      0.0          model_predictions[f\"generated_tokens_{FOLD}\"] = generated_ids_decoded\n",
       "    55        16      20174.0   1260.9      0.0          model_predictions[f\"generated_probas_{FOLD}\"] = probas\n",
       "    56        16   10698428.0 668651.8      0.0          predictions = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
       "    57                                                   \n",
       "    58        16 15984105953.0 999006622.1     21.7          greedy_prefix_decode = greedy_prefix_decoding(used_model, tokenizer, prefix_text, batch[\"input_ids\"], batch[\"attention_mask\"], max_target_length).squeeze().tolist()\n",
       "    59        16    3134614.0 195913.4      0.0          greedy_prefix_predictions = tokenizer.decode(greedy_prefix_decode, skip_special_tokens=True)\n",
       "    60                                                   \n",
       "    61        16 9657533632.0 603595852.0     13.1          one_steps_probas = get_one_step_proba(used_model, \n",
       "    62        16    8466411.0 529150.7      0.0                                                    tokenizer(text_target=[predictions], max_length=max_target_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")[\"input_ids\"], \n",
       "    63        16      54295.0   3393.4      0.0                                                    batch[\"input_ids\"], batch[\"attention_mask\"])\n",
       "    64        16      34442.0   2152.6      0.0          log_proba_of_generated = one_steps_probas[\"log_proba\"]\n",
       "    65        16      45223.0   2826.4      0.0          model_texts[FOLD] = (predictions, greedy_prefix_predictions, log_proba_of_label, log_proba_of_generated)\n",
       "    66                                                   \n",
       "    67         1       2205.0   2205.0      0.0      rouge_scores = dict()\n",
       "    68        18      37552.0   2086.2      0.0      for FOLD, (predictions, greedy_prefix_predictions, log_proba_of_label, log_proba_of_generated) in model_texts.items():\n",
       "    69        18 5906314745.0 328128596.9      8.0          rouge_score = metric.compute(predictions=[predictions], references=[label_text], use_stemmer=True)\n",
       "    70        18      22587.0   1254.8      0.0          rouge_score[\"text\"] = predictions\n",
       "    71        18      22558.0   1253.2      0.0          rouge_score[\"prefix_text\"] = greedy_prefix_predictions\n",
       "    72        18      52728.0   2929.3      0.0          rouge_score[\"prefix_text_match_label\"] = greedy_prefix_predictions.strip().lower() == label_text.strip().lower()\n",
       "    73        18 5886676360.0 327037575.6      8.0          rouge_score[\"rouge_of_prefix_text\"] = metric.compute(predictions=[greedy_prefix_predictions], references=[label_text], use_stemmer=True)\n",
       "    74        18      20301.0   1127.8      0.0          rouge_score[\"log_proba_of_label\"] = log_proba_of_label\n",
       "    75        18      15394.0    855.2      0.0          rouge_score[\"log_proba_of_generated\"] = log_proba_of_generated\n",
       "    76        18      23728.0   1318.2      0.0          rouge_scores[FOLD] = rouge_score\n",
       "    77         1    1770977.0 1770977.0      0.0      rouge_df = pd.DataFrame(rouge_scores.values(), index=rouge_scores.keys())\n",
       "    78         1       1290.0   1290.0      0.0      rouge_df\n",
       "    79         1       1074.0   1074.0      0.0      if fold == \"train\":\n",
       "    80         1    7126512.0 7126512.0      0.0          probas_dict = combined_ds[idx]\n",
       "    81         1       1715.0   1715.0      0.0          pdict = dict()\n",
       "    82                                                   # [x.replace(\"_windowed\", \"\") for x in models.keys() if \"_windowed\" in str(x)] +\n",
       "    83        18      20194.0   1121.9      0.0          for k in [\"proba0\", \"proba1\"] + [x for x in models.keys()]:\n",
       "    84        14      10660.0    761.4      0.0              if k in probas_dict:\n",
       "    85        14      11975.0    855.4      0.0                  pdict[k] = probas_dict[k]\n",
       "    86         1      71683.0  71683.0      0.0          probas_dict = pdict\n",
       "    87                                               else:\n",
       "    88                                                   probas_dict = dict()\n",
       "    89         1      30444.0  30444.0      0.0      our_dict = {\n",
       "    90         1       1039.0   1039.0      0.0                  \"labels\": labels, \n",
       "    91                                                                   # \"label_ids\": label_ids, \n",
       "    92                                                                   # \"pipeline_token_ids\": pipeline_token_ids, \"pipeline_tokens\": pipeline_decoded, \n",
       "    93         1        741.0    741.0      0.0                  **model_predictions}\n",
       "    94                                           \n",
       "    95         1       3760.0   3760.0      0.0      probas_dict.update(our_dict)\n",
       "    96                                               # print({k: len(v) for k, v in probas_dict.items()})\n",
       "    97         1    5716909.0 5716909.0      0.0      df = pd.DataFrame(probas_dict)\n",
       "    98         1    2375098.0 2375098.0      0.0      df = df.loc[((df==tokenizer.pad_token).sum(axis=1) < 6)|(df[\"labels\"]!=tokenizer.pad_token)]\n",
       "    99         1        951.0    951.0      0.0      return rouge_df, df"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# idx = random.randint(0, len(dataset[\"train\"]))\n",
    "# label_text = dataset[\"train\"][idx][\"summary\"]\n",
    "# input_text = f'summarize: {dataset[\"train\"][idx][\"dialogue\"]}'\n",
    "# rouge_df, df = investigate(idx, input_text, label_text, models, generator, tokenizer, gen_kwargs, padding, \"train\", 0.5, do_sample=False, temperature=0.7)\n",
    "%lprun -f investigate investigate(idx, input_text, label_text, models, generator, tokenizer, gen_kwargs, padding, \"train\", 0.5, do_sample=False, temperature=0.7)\n",
    "# rouge_df\n",
    "# print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c3a212c3-647d-432f-969e-d289dd0bf485",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T16:57:12.400017Z",
     "iopub.status.busy": "2023-05-09T16:57:12.399626Z",
     "iopub.status.idle": "2023-05-09T16:57:56.174568Z",
     "shell.execute_reply": "2023-05-09T16:57:56.173644Z",
     "shell.execute_reply.started": "2023-05-09T16:57:12.399988Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "      <th>text</th>\n",
       "      <th>prefix_text</th>\n",
       "      <th>prefix_text_match_label</th>\n",
       "      <th>rouge_of_prefix_text</th>\n",
       "      <th>log_proba_of_label</th>\n",
       "      <th>log_proba_of_generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>input</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.18</td>\n",
       "      <td>summarize: Anna: where are you lost?\\r\\nEma: here only where will i go?\\r\\nAnna: why aint you joining us for girly parties?\\r\\nEma: just been busy with school and job\\r\\nAnna: job? your working?\\r\\nEma: yes during weekends\\r\\nAnna: wow! super girl how do you manage\\r\\nEma: by missing girly parties\\r\\nAnna: awww  why are you working.. its time to enjoy\\r\\nEma: i need money that why\\r\\nAnna: aww.. is everything ok?\\r\\nEma: oh yes absolutely!! its just that i want to start my own business after graduating from university.\\r\\nAnna: wow.. you amaze me every time i talk to you, you are so ambitious\\r\\nEma: thank you, i have very long plans and as for enjoyment and parties are concerned i would better have them with my business associates.\\r\\nAnna: i am so inspired! wish you all the happiness...</td>\n",
       "      <td>Ema is busy with school and work. She is missing out on girly parties. She is saving up to start her own business after graduating. Anna is amazed and inspired by this. Ema finds motivation in</td>\n",
       "      <td>False</td>\n",
       "      <td>{'rouge1': 0.9473684210526316, 'rouge2': 0.945945945945946, 'rougeL': 0.9473684210526316, 'rougeLsum': 0.9473684210526316}</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Ema is busy with school and work. She is missing out on girly parties. She is saving up to start her own business after graduating. Anna is amazed and inspired by this. Ema finds motivation in thinking about her future.</td>\n",
       "      <td>Ema is busy with school and work. She is missing out on girly parties. She is saving up to start her own business after graduating. Anna is amazed and inspired by this. Ema finds motivation in</td>\n",
       "      <td>False</td>\n",
       "      <td>{'rouge1': 0.9473684210526316, 'rouge2': 0.945945945945946, 'rougeL': 0.9473684210526316, 'rougeLsum': 0.9473684210526316}</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>proba_v10_cumulative_windowed</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>Ema has been busy with school and job. She wants to start her own business after graduating from university. She will join Anna for girly parties.</td>\n",
       "      <td>Ema is busy with school and work. She is missing out on girly parties. She is saving up to start her own business after graduating. Anna is amazed and inspired by this. Ema finds motivation in her established</td>\n",
       "      <td>False</td>\n",
       "      <td>{'rouge1': 0.9487179487179489, 'rouge2': 0.9210526315789475, 'rougeL': 0.9487179487179489, 'rougeLsum': 0.9487179487179489}</td>\n",
       "      <td>-88.63</td>\n",
       "      <td>-7.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>proba_v11_cumulative_windowed</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>Ema hasn't been to girly parties because she's been busy with school and job. She wants to start her own business after graduating from university. Anna is inspired by Ema.</td>\n",
       "      <td>Ema is busy with school and work. She is missing out on girly parties. She is saving up to start her own business after graduating. Anna is amazed and inspired by this. Ema finds motivation in her established</td>\n",
       "      <td>False</td>\n",
       "      <td>{'rouge1': 0.9487179487179489, 'rouge2': 0.9210526315789475, 'rougeL': 0.9487179487179489, 'rougeLsum': 0.9487179487179489}</td>\n",
       "      <td>-108.35</td>\n",
       "      <td>-6.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>proba_v12_cumulative_windowed</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.46</td>\n",
       "      <td>Ema is busy with school and job. She wants to start her own business after graduating from university. She will join Anna for girly parties.</td>\n",
       "      <td>Ema is busy with school and work. She is missing out on girly parties. She is saving up to start her own business after graduating. Anna is amazed and inspired by this. Ema finds motivation in her established</td>\n",
       "      <td>False</td>\n",
       "      <td>{'rouge1': 0.9487179487179489, 'rouge2': 0.9210526315789475, 'rougeL': 0.9487179487179489, 'rougeLsum': 0.9487179487179489}</td>\n",
       "      <td>-112.46</td>\n",
       "      <td>-5.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>proba_v10_cumulative_windowed_w10</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>Ema is busy with school and work. She wants to start her own business after graduating from university. Anna is inspired by Ema.</td>\n",
       "      <td>Ema is busy with school and work. She is missing out on girly parties. She is saving up to start her own business after graduating. Anna is amazed and inspired by this. Ema finds motivation in her established business.</td>\n",
       "      <td>False</td>\n",
       "      <td>{'rouge1': 0.9367088607594937, 'rouge2': 0.9090909090909091, 'rougeL': 0.9367088607594937, 'rougeLsum': 0.9367088607594937}</td>\n",
       "      <td>-94.45</td>\n",
       "      <td>-4.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>proba_v12_cumulative_windowed_w5</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>Ema hasn't been to the girly parties because she's been busy with school and job. She wants to start her own business after graduating from university.</td>\n",
       "      <td>Ema is busy with school and work. She is missing out on girly parties. She is saving up to start her own business after graduating. Anna is amazed and inspired by this. Ema finds motivation in her established</td>\n",
       "      <td>False</td>\n",
       "      <td>{'rouge1': 0.9487179487179489, 'rouge2': 0.9210526315789475, 'rougeL': 0.9487179487179489, 'rougeLsum': 0.9487179487179489}</td>\n",
       "      <td>-114.12</td>\n",
       "      <td>-4.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Ema has been working during weekends to earn money for her business. She would rather have parties with her business associates. Anna wishes her good luck and wishes her good grades.</td>\n",
       "      <td>Ema is busy with school and work. She is missing out on girly parties. She is saving up to start her own business after graduating. Anna is amazed and inspired by this. Ema finds motivation in her established</td>\n",
       "      <td>False</td>\n",
       "      <td>{'rouge1': 0.9487179487179489, 'rouge2': 0.9210526315789475, 'rougeL': 0.9487179487179489, 'rougeLsum': 0.9487179487179489}</td>\n",
       "      <td>-98.30</td>\n",
       "      <td>-17.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>Ema has been busy with school and work. She works during weekends to earn money for starting her own business after graduating. She prefers to spend time with her business associates. Anna is impressed with her ambition and wants to be like Ema.</td>\n",
       "      <td>Ema is busy with school and work. She is missing out on girly parties. She is saving up to start her own business after graduating. Anna is amazed and inspired by this. Ema finds motivation in thinking about</td>\n",
       "      <td>False</td>\n",
       "      <td>{'rouge1': 0.9743589743589743, 'rouge2': 0.9736842105263158, 'rougeL': 0.9743589743589743, 'rougeLsum': 0.9743589743589743}</td>\n",
       "      <td>-25.22</td>\n",
       "      <td>-21.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>Ema is working during weekends to earn money for her business. She is not going to the girly parties with Anna and her friends.</td>\n",
       "      <td>Ema is busy with school and work. She is missing out on girly parties. She is saving up to start her own business after graduating. Anna is amazed and inspired by this. Ema finds motivation in her established</td>\n",
       "      <td>False</td>\n",
       "      <td>{'rouge1': 0.9487179487179489, 'rouge2': 0.9210526315789475, 'rougeL': 0.9487179487179489, 'rougeLsum': 0.9487179487179489}</td>\n",
       "      <td>-48.89</td>\n",
       "      <td>-18.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   rouge1  rouge2  rougeL  rougeLsum  \\\n",
       "input                                0.23    0.08    0.21       0.18   \n",
       "label                                1.00    1.00    1.00       1.00   \n",
       "proba_v10_cumulative_windowed        0.52    0.31    0.42       0.42   \n",
       "proba_v11_cumulative_windowed        0.58    0.34    0.50       0.50   \n",
       "proba_v12_cumulative_windowed        0.55    0.38    0.46       0.46   \n",
       "proba_v10_cumulative_windowed_w10    0.63    0.49    0.63       0.63   \n",
       "proba_v12_cumulative_windowed_w5     0.47    0.30    0.38       0.38   \n",
       "0                                    0.34    0.00    0.25       0.25   \n",
       "1                                    0.48    0.27    0.43       0.43   \n",
       "baseline                             0.44    0.10    0.31       0.31   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              text  \\\n",
       "input                              summarize: Anna: where are you lost?\\r\\nEma: here only where will i go?\\r\\nAnna: why aint you joining us for girly parties?\\r\\nEma: just been busy with school and job\\r\\nAnna: job? your working?\\r\\nEma: yes during weekends\\r\\nAnna: wow! super girl how do you manage\\r\\nEma: by missing girly parties\\r\\nAnna: awww  why are you working.. its time to enjoy\\r\\nEma: i need money that why\\r\\nAnna: aww.. is everything ok?\\r\\nEma: oh yes absolutely!! its just that i want to start my own business after graduating from university.\\r\\nAnna: wow.. you amaze me every time i talk to you, you are so ambitious\\r\\nEma: thank you, i have very long plans and as for enjoyment and parties are concerned i would better have them with my business associates.\\r\\nAnna: i am so inspired! wish you all the happiness...   \n",
       "label                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Ema is busy with school and work. She is missing out on girly parties. She is saving up to start her own business after graduating. Anna is amazed and inspired by this. Ema finds motivation in thinking about her future.    \n",
       "proba_v10_cumulative_windowed                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Ema has been busy with school and job. She wants to start her own business after graduating from university. She will join Anna for girly parties.   \n",
       "proba_v11_cumulative_windowed                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Ema hasn't been to girly parties because she's been busy with school and job. She wants to start her own business after graduating from university. Anna is inspired by Ema.   \n",
       "proba_v12_cumulative_windowed                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Ema is busy with school and job. She wants to start her own business after graduating from university. She will join Anna for girly parties.   \n",
       "proba_v10_cumulative_windowed_w10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Ema is busy with school and work. She wants to start her own business after graduating from university. Anna is inspired by Ema.   \n",
       "proba_v12_cumulative_windowed_w5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Ema hasn't been to the girly parties because she's been busy with school and job. She wants to start her own business after graduating from university.   \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Ema has been working during weekends to earn money for her business. She would rather have parties with her business associates. Anna wishes her good luck and wishes her good grades.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Ema has been busy with school and work. She works during weekends to earn money for starting her own business after graduating. She prefers to spend time with her business associates. Anna is impressed with her ambition and wants to be like Ema.   \n",
       "baseline                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Ema is working during weekends to earn money for her business. She is not going to the girly parties with Anna and her friends.   \n",
       "\n",
       "                                                                                                                                                                                                                                                  prefix_text  \\\n",
       "input                                                        Ema is busy with school and work. She is missing out on girly parties. She is saving up to start her own business after graduating. Anna is amazed and inspired by this. Ema finds motivation in   \n",
       "label                                                        Ema is busy with school and work. She is missing out on girly parties. She is saving up to start her own business after graduating. Anna is amazed and inspired by this. Ema finds motivation in   \n",
       "proba_v10_cumulative_windowed                Ema is busy with school and work. She is missing out on girly parties. She is saving up to start her own business after graduating. Anna is amazed and inspired by this. Ema finds motivation in her established   \n",
       "proba_v11_cumulative_windowed                Ema is busy with school and work. She is missing out on girly parties. She is saving up to start her own business after graduating. Anna is amazed and inspired by this. Ema finds motivation in her established   \n",
       "proba_v12_cumulative_windowed                Ema is busy with school and work. She is missing out on girly parties. She is saving up to start her own business after graduating. Anna is amazed and inspired by this. Ema finds motivation in her established   \n",
       "proba_v10_cumulative_windowed_w10  Ema is busy with school and work. She is missing out on girly parties. She is saving up to start her own business after graduating. Anna is amazed and inspired by this. Ema finds motivation in her established business.   \n",
       "proba_v12_cumulative_windowed_w5             Ema is busy with school and work. She is missing out on girly parties. She is saving up to start her own business after graduating. Anna is amazed and inspired by this. Ema finds motivation in her established   \n",
       "0                                            Ema is busy with school and work. She is missing out on girly parties. She is saving up to start her own business after graduating. Anna is amazed and inspired by this. Ema finds motivation in her established   \n",
       "1                                             Ema is busy with school and work. She is missing out on girly parties. She is saving up to start her own business after graduating. Anna is amazed and inspired by this. Ema finds motivation in thinking about   \n",
       "baseline                                     Ema is busy with school and work. She is missing out on girly parties. She is saving up to start her own business after graduating. Anna is amazed and inspired by this. Ema finds motivation in her established   \n",
       "\n",
       "                                   prefix_text_match_label  \\\n",
       "input                                                False   \n",
       "label                                                False   \n",
       "proba_v10_cumulative_windowed                        False   \n",
       "proba_v11_cumulative_windowed                        False   \n",
       "proba_v12_cumulative_windowed                        False   \n",
       "proba_v10_cumulative_windowed_w10                    False   \n",
       "proba_v12_cumulative_windowed_w5                     False   \n",
       "0                                                    False   \n",
       "1                                                    False   \n",
       "baseline                                             False   \n",
       "\n",
       "                                                                                                                                          rouge_of_prefix_text  \\\n",
       "input                               {'rouge1': 0.9473684210526316, 'rouge2': 0.945945945945946, 'rougeL': 0.9473684210526316, 'rougeLsum': 0.9473684210526316}   \n",
       "label                               {'rouge1': 0.9473684210526316, 'rouge2': 0.945945945945946, 'rougeL': 0.9473684210526316, 'rougeLsum': 0.9473684210526316}   \n",
       "proba_v10_cumulative_windowed      {'rouge1': 0.9487179487179489, 'rouge2': 0.9210526315789475, 'rougeL': 0.9487179487179489, 'rougeLsum': 0.9487179487179489}   \n",
       "proba_v11_cumulative_windowed      {'rouge1': 0.9487179487179489, 'rouge2': 0.9210526315789475, 'rougeL': 0.9487179487179489, 'rougeLsum': 0.9487179487179489}   \n",
       "proba_v12_cumulative_windowed      {'rouge1': 0.9487179487179489, 'rouge2': 0.9210526315789475, 'rougeL': 0.9487179487179489, 'rougeLsum': 0.9487179487179489}   \n",
       "proba_v10_cumulative_windowed_w10  {'rouge1': 0.9367088607594937, 'rouge2': 0.9090909090909091, 'rougeL': 0.9367088607594937, 'rougeLsum': 0.9367088607594937}   \n",
       "proba_v12_cumulative_windowed_w5   {'rouge1': 0.9487179487179489, 'rouge2': 0.9210526315789475, 'rougeL': 0.9487179487179489, 'rougeLsum': 0.9487179487179489}   \n",
       "0                                  {'rouge1': 0.9487179487179489, 'rouge2': 0.9210526315789475, 'rougeL': 0.9487179487179489, 'rougeLsum': 0.9487179487179489}   \n",
       "1                                  {'rouge1': 0.9743589743589743, 'rouge2': 0.9736842105263158, 'rougeL': 0.9743589743589743, 'rougeLsum': 0.9743589743589743}   \n",
       "baseline                           {'rouge1': 0.9487179487179489, 'rouge2': 0.9210526315789475, 'rougeL': 0.9487179487179489, 'rougeLsum': 0.9487179487179489}   \n",
       "\n",
       "                                   log_proba_of_label  log_proba_of_generated  \n",
       "input                                            0.00                    0.00  \n",
       "label                                            0.00                    0.00  \n",
       "proba_v10_cumulative_windowed                  -88.63                   -7.74  \n",
       "proba_v11_cumulative_windowed                 -108.35                   -6.96  \n",
       "proba_v12_cumulative_windowed                 -112.46                   -5.24  \n",
       "proba_v10_cumulative_windowed_w10              -94.45                   -4.85  \n",
       "proba_v12_cumulative_windowed_w5              -114.12                   -4.35  \n",
       "0                                              -98.30                  -17.82  \n",
       "1                                              -25.22                  -21.52  \n",
       "baseline                                       -48.89                  -18.75  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "idx = random.randint(0, len(dataset[\"train\"]))\n",
    "label_text = dataset[\"train\"][idx][\"summary\"]\n",
    "input_text = f'summarize: {dataset[\"train\"][idx][\"dialogue\"]}'\n",
    "rouge_df, df = investigate(idx, input_text, label_text, models, generator, tokenizer, gen_kwargs, padding, \"train\", 0.9, do_sample=False, temperature=0.7)\n",
    "rouge_df\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd909453-22ba-4702-a5d7-7984c7424e36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k in range(20):\n",
    "    idx = random.randint(0, len(dataset[\"train\"]))\n",
    "    label_text = dataset[\"train\"][idx][\"summary\"]\n",
    "    input_text = f'summarize: {dataset[\"train\"][idx][\"dialogue\"]}'\n",
    "    rouge_df, df = investigate(idx, input_text, label_text, models, generator, tokenizer, gen_kwargs, padding, \"train\", do_sample=False)\n",
    "    rouge_df\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffaea14-e937-4107-8ae3-529b10481ba6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k in range(5):\n",
    "    idx = random.randint(0, len(dataset[\"train\"]))\n",
    "    label_text = dataset[\"train\"][idx][\"summary\"]\n",
    "    input_text = f'summarize: {dataset[\"train\"][idx][\"dialogue\"]}'\n",
    "    rouge_df, df = investigate(idx, input_text, label_text, models, generator, tokenizer, gen_kwargs, padding, \"train\", do_sample=False, num_beams=10, temperature=0.6)\n",
    "    rouge_df\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fab0bc-523e-4cce-bbd6-0421c336bd9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302abedb-fd30-4a72-a71b-843e24060184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dca748-dabb-4c32-ad51-9853b537b960",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = 3\n",
    "label_text = dataset[\"train\"][idx][\"summary\"]\n",
    "input_text = f'summarize: {dataset[\"train\"][idx][\"dialogue\"]}'\n",
    "rouge_df, df = investigate(idx, input_text, label_text, models, generator, tokenizer, gen_kwargs, padding, \"train\")\n",
    "\n",
    "rouge_df\n",
    "# df\n",
    "\n",
    "# do_sample=False, num_beams=1, temperature=0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a056743d-af03-4950-a335-0dc235ce0561",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T14:08:36.755605Z",
     "iopub.status.busy": "2023-05-04T14:08:36.755173Z",
     "iopub.status.idle": "2023-05-04T14:08:36.762507Z",
     "shell.execute_reply": "2023-05-04T14:08:36.761854Z",
     "shell.execute_reply.started": "2023-05-04T14:08:36.755576Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Edward: Rachel, I think I'm in ove with Bella..\\r\\nrachel: Dont say anything else..\\r\\nEdward: What do you mean??\\r\\nrachel: Open your fu**ing door.. I'm outside\""
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Edward thinks he is in love with Bella. Rachel wants Edward to open his door. Rachel is outside. '"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][idx][\"dialogue\"]\n",
    "dataset[\"train\"][idx][\"summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00758eaa-51ba-491a-8f3b-e2c22e04adc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-07T10:03:39.564917Z",
     "iopub.status.busy": "2023-05-07T10:03:39.564496Z",
     "iopub.status.idle": "2023-05-07T10:03:39.570442Z",
     "shell.execute_reply": "2023-05-07T10:03:39.569787Z",
     "shell.execute_reply.started": "2023-05-07T10:03:39.564883Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = 3\n",
    "label_text = dataset[\"train\"][idx][\"summary\"]\n",
    "altered_label_text = 'Rajeev thinks he is in love with Rashmi. Sima wants Edward to open his door. Sima is outside. '\n",
    "input_text = f'summarize: {dataset[\"train\"][idx][\"dialogue\"]}'\n",
    "\n",
    "\n",
    "altered_input_text = \"Rajeev: Sima, I think I'm in ove with Rashmi..\\r\\nsima: Dont say anything else..\\r\\nRajeev: What do you mean??\\r\\nsima: Open your fu**ing door.. I'm outside\"\n",
    "altered_input_text = f'summarize: {altered_input_text}'\n",
    "\n",
    "rouge_df, df = investigate(idx, altered_input_text, altered_label_text, models, generator, tokenizer, gen_kwargs, padding, \"train\")\n",
    "\n",
    "rouge_df\n",
    "df\n",
    "\n",
    "# do_sample=False, num_beams=1, temperature=0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1110123f-f30c-4ca0-8880-a7c25e7c5600",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_df = df.select_dtypes(include=['float64'])\n",
    "\n",
    "probas_df_only = check_df\n",
    "probas_df_only.index = df[\"labels\"]\n",
    "plot_heatmap(probas_df_only, figsize=(14,14))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8b421f-5e1d-40e7-959a-d201026449ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_correlation_heatmap(probas_df_only, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5afb2353-a641-459b-8774-6036a0feca5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-07T10:04:07.771061Z",
     "iopub.status.busy": "2023-05-07T10:04:07.770661Z",
     "iopub.status.idle": "2023-05-07T10:04:07.783559Z",
     "shell.execute_reply": "2023-05-07T10:04:07.782943Z",
     "shell.execute_reply.started": "2023-05-07T10:04:07.771032Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 8200,   317,     7,     3,    88,    19,    16,   333,    28,  5377,\n",
       "             9,     5, 15868,  2746,  8200,    12,   539,   112,  1365,     5,\n",
       "         15868,    19,  1067,     5,     1,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    0,  8200,   317,     7,     3,    88,    19,    16,   333,    28,\n",
       "          5377,     9,     5, 15868,  2746,  8200,    12,   539,   112,  1365,\n",
       "             5, 15868,    19,  1067,     5,     1,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(text_target=[input_text], max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "labels = tokenizer(text_target=[label_text], max_length=128, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "label_ids = labels[\"input_ids\"]\n",
    "label_ids.shape\n",
    "label_ids\n",
    "\n",
    "model._shift_right(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f6a48ca6-4aae-417d-b15d-55880e914b1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T08:57:07.421758Z",
     "iopub.status.busy": "2023-05-04T08:57:07.421378Z",
     "iopub.status.idle": "2023-05-04T08:57:08.743959Z",
     "shell.execute_reply": "2023-05-04T08:57:08.743167Z",
     "shell.execute_reply.started": "2023-05-04T08:57:07.421730Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-22.270734582341067"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-11.429230845551135"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_one_step_proba(models[0], label_ids, input_ids[\"input_ids\"], input_ids[\"attention_mask\"])[\"log_proba\"]\n",
    "get_one_step_proba(models[1], label_ids, input_ids[\"input_ids\"], input_ids[\"attention_mask\"])[\"log_proba\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eec9e5f3-0bac-4a03-b72f-b224c00a811e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-07T10:06:40.850073Z",
     "iopub.status.busy": "2023-05-07T10:06:40.849774Z",
     "iopub.status.idle": "2023-05-07T10:06:40.855234Z",
     "shell.execute_reply": "2023-05-07T10:06:40.854596Z",
     "shell.execute_reply.started": "2023-05-07T10:06:40.850047Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, None, 0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id, tokenizer.bos_token_id, tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4d9b1180-d6d1-4d9a-aa4e-4213d157ed33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-07T11:18:43.451180Z",
     "iopub.status.busy": "2023-05-07T11:18:43.450608Z",
     "iopub.status.idle": "2023-05-07T11:18:43.459758Z",
     "shell.execute_reply": "2023-05-07T11:18:43.459118Z",
     "shell.execute_reply.started": "2023-05-07T11:18:43.451133Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1664129c-3e73-4e12-bd33-2d7d09b36a5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-07T11:18:59.849916Z",
     "iopub.status.busy": "2023-05-07T11:18:59.849501Z",
     "iopub.status.idle": "2023-05-07T11:19:02.899838Z",
     "shell.execute_reply": "2023-05-07T11:19:02.898961Z",
     "shell.execute_reply.started": "2023-05-07T11:18:59.849886Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Edward thinks he is in love with Bella. Rachel wants Edward to open his door. Rachel is outside. '"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Edward thinks he is '"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Edward thinks he is in love with Bella. Rachel is outside.</s>']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Edward thinks he is in love with Bella. Rachel is outside, but Edward is outside.</s>']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Edward thinks he is in love with Bella. Rachel is outside, but Edward is outside.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 3\n",
    "label_text = dataset[\"train\"][idx][\"summary\"]\n",
    "input_text = f'summarize: {dataset[\"train\"][idx][\"dialogue\"]}'\n",
    "input_ids = tokenizer(text_target=[input_text], max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "labels = tokenizer(text_target=[label_text], max_length=max_target_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "label_ids = labels[\"input_ids\"]\n",
    "\n",
    "prefix_text = \" \".join(label_text.split()[:4]) + \" \"\n",
    "label_text\n",
    "prefix_text\n",
    "print(\"=\"*40)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "greedy_decode = greedy_prefix_decoding(models[1], tokenizer, prefix_text, input_ids[\"input_ids\"], input_ids[\"attention_mask\"], max_target_length, )\n",
    "tokenizer.batch_decode(greedy_decode)\n",
    "\n",
    "greedy_decode = greedy_prefix_decoding(models[0], tokenizer, prefix_text, input_ids[\"input_ids\"], input_ids[\"attention_mask\"], max_target_length,)\n",
    "tokenizer.batch_decode(greedy_decode)\n",
    "tokenizer.decode(greedy_decode.squeeze().tolist(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5555f83-8562-4885-95db-ca27d992e4fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
